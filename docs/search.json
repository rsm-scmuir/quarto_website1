[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scotland’s Website",
    "section": "",
    "text": "Welcome to my website!\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "My Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects/project1/HW1/hw1_questions.html",
    "href": "projects/project1/HW1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse. In a large-scale natural field experiment, Dean Karlan and John List partnered with a liberal nonprofit organization (that focuses on Civil Rights) in the United States to investigate how matching gift offers affect charitable giving. The donations will be tax-deuctible for federal income taxes. The study involved 50,083 individuals who had donated to the organization at least once since 1991. These prior donors were randomly divided into a control group and a treatment group. The control group received a standard direct mail fundraising letter, while the treatment group received an otherwise identical letter that included a matching grant offer from a “concerned fellow member.” Within the treatment group, participants were further randomly assigned to different variations across three key dimensions: (1) the match ratio—$1:$1, $2:$1, or $3:$1; (2) the maximum amount available from the matching donor—$25,000, $50,000, $100,000, or unspecified; and (3) the suggested donation amount, which was either equal to, 1.25 times, or 1.5 times the recipient’s previous highest contribution. These design elements allowed the researchers to isolate and test the effects of both the presence and structure of matching incentives on giving behavior. All letters were mailed in August 2005, and the fundraising appeal was tied to a current political issue at the time—Supreme Court nominations—to enhance relevance and urgency.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/HW1/hw1_questions.html#introduction",
    "href": "projects/project1/HW1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse. In a large-scale natural field experiment, Dean Karlan and John List partnered with a liberal nonprofit organization (that focuses on Civil Rights) in the United States to investigate how matching gift offers affect charitable giving. The donations will be tax-deuctible for federal income taxes. The study involved 50,083 individuals who had donated to the organization at least once since 1991. These prior donors were randomly divided into a control group and a treatment group. The control group received a standard direct mail fundraising letter, while the treatment group received an otherwise identical letter that included a matching grant offer from a “concerned fellow member.” Within the treatment group, participants were further randomly assigned to different variations across three key dimensions: (1) the match ratio—$1:$1, $2:$1, or $3:$1; (2) the maximum amount available from the matching donor—$25,000, $50,000, $100,000, or unspecified; and (3) the suggested donation amount, which was either equal to, 1.25 times, or 1.5 times the recipient’s previous highest contribution. These design elements allowed the researchers to isolate and test the effects of both the presence and structure of matching incentives on giving behavior. All letters were mailed in August 2005, and the fundraising appeal was tied to a current political issue at the time—Supreme Court nominations—to enhance relevance and urgency.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/HW1/hw1_questions.html#data",
    "href": "projects/project1/HW1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset includes donation outcomes (whether and how much was donated), treatment assignments (match ratio, maximum match amount, suggested donation), and individual donor history (e.g., prior donation amount and frequency). It also incorporates ZIP code-level demographics (income, education, race, household size), political context (red/blue state, voted for bush, and county), and nonprofit activity levels by state. This structure enables analysis of treatment effects and heterogeneity across political and demographic groups.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\nimport pandas as pd\ndf = pd.read_stata(\"/home/jovyan/Desktop/quarto_website1/projects/project1/HW1/karlan_list_2007.dta\")\ndf['ratio1'] = (df['ratio'] == 1).astype(int)\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport numpy as np\nimport statsmodels.formula.api as smf\n\ndef welch_t_test(x, y):\n    x = x.dropna()\n    y = y.dropna()\n    n_x, n_y = len(x), len(y)\n    mean_x, mean_y = np.mean(x), np.mean(y)\n    var_x, var_y = np.var(x, ddof=1), np.var(y, ddof=1)\n    denominator = np.sqrt((var_x / n_x) + (var_y / n_y))\n    if denominator == 0:\n        return np.nan\n    return (mean_x - mean_y) / denominator\n\ncovariates = [\n    'mrm2', 'hpa', 'years', 'female', 'couple',\n    'pwhite', 'pblack', 'ave_hh_sz', 'median_hhincome'\n]\n\nfor var in covariates:\n    print(f\"\\n--- Manual Welch t-test for {var} ---\")\n    x = df[df['treatment'] == 1][var]\n    y = df[df['treatment'] == 0][var]\n    t_stat = welch_t_test(x, y)\n    print(f\"t = {t_stat:.4f}\")\n\n    # Linear regression comparison\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    coef = model.params['treatment']\n    p = model.pvalues['treatment']\n    print(f\"Regression: coef = {coef:.3f}, p = {p:.4f}\")\n\n\n--- Manual Welch t-test for mrm2 ---\nt = 0.1195\nRegression: coef = 0.014, p = 0.9049\n\n--- Manual Welch t-test for hpa ---\nt = 0.9704\nRegression: coef = 0.637, p = 0.3451\n\n--- Manual Welch t-test for years ---\nt = -1.0909\nRegression: coef = -0.058, p = 0.2700\n\n--- Manual Welch t-test for female ---\nt = -1.7535\nRegression: coef = -0.008, p = 0.0787\n\n--- Manual Welch t-test for couple ---\nt = -0.5823\nRegression: coef = -0.002, p = 0.5594\n\n--- Manual Welch t-test for pwhite ---\nt = -0.5590\nRegression: coef = -0.001, p = 0.5753\n\n--- Manual Welch t-test for pblack ---\nt = 0.0975\nRegression: coef = 0.000, p = 0.9219\n\n--- Manual Welch t-test for ave_hh_sz ---\nt = 0.8234\nRegression: coef = 0.003, p = 0.4098\n\n--- Manual Welch t-test for median_hhincome ---\nt = -0.7433\nRegression: coef = -157.925, p = 0.4583\n\n\n\n\nBalance Test Results Summary\nThe Welch t-tests and linear regressions across the selected covariates indicate that there are no statistically significant differences between the treatment and control groups. This suggests that the randomization process successfully created. These balance test results support the internal validity of the experimental design by confirming that any observed differences in outcomes can likely be attributed to the treatment rather than to pre-existing group differences."
  },
  {
    "objectID": "projects/project1/HW1/hw1_questions.html#experimental-results",
    "href": "projects/project1/HW1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\ngroup_totals = df.groupby('treatment')['amount'].sum()\ngroup_totals.index = ['Control', 'Treatment']\nplt.figure(figsize=(6, 5))\ngroup_totals.plot(kind='bar', color=['skyblue', 'salmon'])\nplt.title('Total Donations Raised by Group')\nplt.ylabel('Total Dollars Raised')\nplt.xlabel('Group')\nplt.xticks(rotation=0)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\n\ncontrol_gave = df[df['treatment'] == 0]['gave'].dropna()\ntreatment_gave = df[df['treatment'] == 1]['gave'].dropna()\n\nt_stat, p_val = ttest_ind(treatment_gave, control_gave, equal_var=False)\nprint(f\"T-test: t = {t_stat:.3f}, p = {p_val:.4f}\")\n\nT-test: t = 3.209, p = 0.0013\n\n\nThe t-test shows a statistically significant difference in donation rates between the treatment and control groups (t = 3.209, p = 0.0013), supporting the hypothesis that offering a matching donation increases the likelihood of giving. This suggests that people are more inclined to donate when they perceive their contribution to have greater impact, highlighting how behavioral cues like matching grants can effectively nudge charitable behavior.\n\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols('gave ~ treatment', data=df).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Tue, 10 Jun 2025   Prob (F-statistic):            0.00193\nTime:                        18:14:58   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nmy results do compare to table 3\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nfrom scipy.stats import ttest_ind\n\nr1 = df[df['ratio'] == 1]['gave']\nr2 = df[df['ratio2'] == 1]['gave']\nr3 = df[df['ratio3'] == 1]['gave']\n\nprint(\"T-test: 2:1 vs 1:1\")\nprint(ttest_ind(r2, r1, equal_var=False))\n\nprint(\"\\nT-test: 3:1 vs 1:1\")\nprint(ttest_ind(r3, r1, equal_var=False))\n\nprint(\"\\nT-test: 3:1 vs 2:1\")\nprint(ttest_ind(r3, r2, equal_var=False))\n\nT-test: 2:1 vs 1:1\nTtestResult(statistic=0.965048975142932, pvalue=0.33453078237183076, df=22225.07770983836)\n\nT-test: 3:1 vs 1:1\nTtestResult(statistic=1.0150174470156275, pvalue=0.31010856527625774, df=22215.0529778684)\n\nT-test: 3:1 vs 2:1\nTtestResult(statistic=0.05011581369764474, pvalue=0.9600305476940865, df=22260.84918918778)\n\n\nYes these results support what the authors said on page 8\n\nInterpretation\nmy results show no statistically significant difference in donation rates between any of the match ratios tested. In other words, increasing the match ratio from 1:1 to 2:1 or 3:1 did not make people more likely to donate.\n\nimport statsmodels.formula.api as smf\nmodel = smf.ols('gave ~ ratio1 + ratio2 + ratio3', data=df).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Tue, 10 Jun 2025   Prob (F-statistic):             0.0118\nTime:                        18:14:58   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe regression shows that 2:1 and 3:1 match ratios significantly increase donation rates compared to the base group. However, their effects are nearly identical, suggesting no added benefit from raising the match beyond 2:1. This supports the paper’s conclusion that while matching boosts giving, higher ratios don’t lead to more donations.\n\ncoef_r2 = model.params.get('ratio2', 0)\ncoef_r3 = model.params.get('ratio3', 0)\nprint(f\"\\nDifference 2:1 vs 1:1: {coef_r2:.4f}\")\nprint(f\"Difference 3:1 vs 1:1: {coef_r3:.4f}\")\nprint(f\"Difference 3:1 vs 2:1: {coef_r3 - coef_r2:.4f}\")\n\n\nDifference 2:1 vs 1:1: 0.0048\nDifference 3:1 vs 1:1: 0.0049\nDifference 3:1 vs 2:1: 0.0001\n\n\nThe regression shows that both 2:1 and 3:1 match ratios increase donation rates by about 0.5 percentage points compared to 1:1. But there’s almost no difference between 2:1 and 3:1, so bigger match ratios don’t seem to help more. It looks like just having a match matters more than how big it is.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\ncontrol_amt = df[df['treatment'] == 0]['amount']\ntreatment_amt = df[df['treatment'] == 1]['amount']\nt_stat, p_val = ttest_ind(treatment_amt, control_amt, equal_var=False)\nprint(f\"T-test (all data): t = {t_stat:.3f}, p = {p_val:.4f}\")\n\nT-test (all data): t = 1.918, p = 0.0551\n\n\nThe t-test shows a small difference in donation amounts between the treatment and control groups, with the treatment group giving slightly more on average. However, the result is not statistically significant (p = 0.0551), meaning we cannot confidently conclude that the treatment had an effect on how much people donated.\n\ndf_donors = df[df['amount'] &gt; 0]\n\nc_amt = df_donors[df_donors['treatment'] == 0]['amount']\nt_amt = df_donors[df_donors['treatment'] == 1]['amount']\nt_stat, p_val = ttest_ind(t_amt, c_amt, equal_var=False)\nprint(f\"T-test (donors only): t = {t_stat:.3f}, p = {p_val:.4f}\")\n\nmodel_donors = smf.ols('amount ~ treatment', data=df_donors).fit()\nprint(model_donors.summary())\n\nT-test (donors only): t = -0.585, p = 0.5590\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Tue, 10 Jun 2025   Prob (F-statistic):              0.561\nTime:                        18:14:58   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe regression shows that among those who donated, individuals in the treatment group gave about $1.67 less than those in the control group, but this difference is not statistically significant (p = 0.561). This suggests that while matching offers increase the likelihood of donating, they do not affect the donation amount once a person chooses to give. Therefore, we cannot draw a causal conclusion about the treatment’s impact on contribution size.\n\nimport matplotlib.pyplot as plt\n\n\ncontrol = df_donors[df_donors['treatment'] == 0]['amount']\ntreatment = df_donors[df_donors['treatment'] == 1]['amount']\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\naxes[0].hist(control, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(control.mean(), color='red', linestyle='dashed', label=f'Mean: ${control.mean():.2f}')\naxes[0].set_title('Control Group - Donation Amounts')\naxes[0].set_xlabel('Donation Amount ($)')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\naxes[1].hist(treatment, bins=30, color='salmon', edgecolor='black')\naxes[1].axvline(treatment.mean(), color='red', linestyle='dashed', label=f'Mean: ${treatment.mean():.2f}')\naxes[1].set_title('Treatment Group - Donation Amounts')\naxes[1].set_xlabel('Donation Amount ($)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/project1/HW1/hw1_questions.html#simulation-experiment",
    "href": "projects/project1/HW1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nn = 10000\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control\n\ncontrol_draws = np.random.binomial(n=1, p=p_control, size=n)\ntreatment_draws = np.random.binomial(n=1, p=p_treatment, size=n)\n\ndifferences = treatment_draws - control_draws\n\ncumulative_avg_diff = np.cumsum(differences) / np.arange(1, n + 1)\n\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg_diff, label='Cumulative Avg. Difference')\nplt.axhline(true_diff, color='red', linestyle='dashed', label='True Difference = 0.004')\nplt.title(\"Law of Large Numbers: Cumulative Avg. Difference in Donation Rates\")\nplt.xlabel(\"Number of Simulated Pairs\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis simulation shows how the average difference in donation rates between treatment and control groups stabilizes as the sample size increases. Although early values fluctuate due to randomness, the cumulative average converges to the true difference (0.004) as more data is added. This demonstrates the Law of Large Numbers: with enough observations, sample averages reliably approach their expected values.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ncontrol_p = 0.018\ntreat_p = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_simulations):\n        control = np.random.binomial(1, control_p, n)\n        treat = np.random.binomial(1, treat_p, n)\n        diffs.append(np.mean(treat) - np.mean(control))\n    \n \n    axes[i].hist(diffs, bins=30, color='skyblue', edgecolor='black')\n    axes[i].axvline(0, color='red', linestyle='dashed', label='Zero')\n    axes[i].set_title(f'Sample Size = {n}')\n    axes[i].set_xlabel('Avg. Difference in Means')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs sample size increases, the distribution of average differences becomes more concentrated and symmetric, illustrating the Central Limit Theorem. With small samples (e.g., 50), the spread is wide and variable. But by n = 1000, the distribution is nearly normal and centered around the true mean difference. Importantly, zero is in the tail, not the center, suggesting that the effect of the treatment is consistently positive across simulations."
  },
  {
    "objectID": "projects/project1/HW3/hw3_questions.html",
    "href": "projects/project1/HW3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/project1/HW3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/project1/HW3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/project1/HW3/hw3_questions.html#simulate-conjoint-data",
    "href": "projects/project1/HW3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "projects/project1/HW3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "projects/project1/HW3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables."
  },
  {
    "objectID": "projects/project1/HW3/hw3_questions.html#preparing-the-data-for-estimation-1",
    "href": "projects/project1/HW3/hw3_questions.html#preparing-the-data-for-estimation-1",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nIn this section, we load and preprocess our conjoint dataset to be compatible with multinomial logit estimation. Each row represents an alternative in a choice task, and we encode categorical variables into binary indicators.\n\n# Load required library\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Load the CSV data\nconjoint_data &lt;- read.csv(\"/home/jovyan/Desktop/quarto_website1/projects/project1/HW3/conjoint_data.csv\", \n                          stringsAsFactors = FALSE)\n\n# Convert columns to proper types\nconjoint_data &lt;- conjoint_data %&gt;%\n  mutate(\n    respondent = as.integer(resp),\n    task = as.integer(task),\n    choice = as.integer(choice),\n    price = as.numeric(price),\n    brand = factor(brand),\n    ads = factor(ad)\n  ) %&gt;%\n  select(respondent, task, choice, brand, ads, price)\n\n# Create design matrix (excluding intercept)\nX &lt;- model.matrix(~ brand + ads + price, data = conjoint_data)\n\n# Combine with respondent, task, and choice\ndata_prepped &lt;- cbind(conjoint_data[, c(\"respondent\", \"task\", \"choice\")], X[, -1])\n\n# Preview result\nhead(data_prepped)\n\n  respondent task choice brandN brandP adsYes price\n1          1    1      1      1      0      1    28\n2          1    1      0      0      0      1    16\n3          1    1      0      0      1      1    16\n4          1    2      0      1      0      1    32\n5          1    2      1      0      1      1    16\n6          1    2      0      1      0      1    24"
  },
  {
    "objectID": "projects/project1/HW3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "projects/project1/HW3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\n## 4. Estimation via Maximum Likelihood\n\n# Load required libraries\nlibrary(dplyr)\n\n# Log-likelihood function for MNL model\nmnl_loglik &lt;- function(beta, X, y, id_task) {\n  df &lt;- data.frame(\n    util = as.vector(X %*% beta),\n    choice = y,\n    task = id_task\n  )\n\n  log_likelihood &lt;- df %&gt;%\n    group_by(task) %&gt;%\n    summarise(log_prob = {\n      exp_util &lt;- exp(util)\n      prob &lt;- exp_util / sum(exp_util)\n      log(prob[choice == 1])\n    }) %&gt;%\n    pull(log_prob) %&gt;%\n    sum()\n\n  return(-log_likelihood)  # Return negative log-likelihood for minimization\n}\n\n# Prepare inputs\nX &lt;- as.matrix(data_prepped[, c(\"brandN\", \"brandP\", \"adsYes\", \"price\")])\ny &lt;- data_prepped$choice\nid_task &lt;- paste(data_prepped$respondent, data_prepped$task, sep = \"_\")\nstart_vals &lt;- rep(0, ncol(X))\n\n# Estimate MLEs using optim\nmle_fit &lt;- optim(\n  par = start_vals,\n  fn = mnl_loglik,\n  X = X,\n  y = y,\n  id_task = id_task,\n  method = \"BFGS\",\n  hessian = TRUE\n)\n\n# Extract estimates and compute standard errors\ncoef_est &lt;- mle_fit$par\nvcov_mat &lt;- solve(mle_fit$hessian)\nse_est &lt;- sqrt(diag(vcov_mat))\n\n# Construct 95% confidence intervals\nconf_int &lt;- cbind(\n  Estimate = coef_est,\n  `Std. Error` = se_est,\n  `CI Lower` = coef_est - 1.96 * se_est,\n  `CI Upper` = coef_est + 1.96 * se_est\n)\n\n# Round and label\nconf_int &lt;- round(conf_int, 4)\nrownames(conf_int) &lt;- colnames(X)\n\n# Display result\nconf_int\n\n       Estimate Std. Error CI Lower CI Upper\nbrandN   0.9412     0.1110   0.7236   1.1588\nbrandP   0.5016     0.1111   0.2839   0.7194\nadsYes  -0.7320     0.0878  -0.9041  -0.5599\nprice   -0.0995     0.0063  -0.1119  -0.0871"
  },
  {
    "objectID": "projects/project1/HW3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "projects/project1/HW3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\n\n## 5. Estimation via Bayesian Methods\n\nlibrary(MASS)  # for mvrnorm\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n# Reuse log-likelihood from MLE section (with X, y, id_task)\nlog_lik &lt;- function(beta) {\n  df &lt;- data.frame(util = as.vector(X %*% beta), choice = y, task = id_task)\n  \n  df %&gt;%\n    group_by(task) %&gt;%\n    summarise(log_prob = {\n      exp_util &lt;- exp(util)\n      prob &lt;- exp_util / sum(exp_util)\n      log(prob[choice == 1])\n    }) %&gt;%\n    pull(log_prob) %&gt;%\n    sum()\n}\n\n# Log-prior: N(0, 5^2) for first 3, N(0, 1^2) for price\nlog_prior &lt;- function(beta) {\n  sum(dnorm(beta[1:3], mean = 0, sd = 5, log = TRUE)) +\n    dnorm(beta[4], mean = 0, sd = 1, log = TRUE)\n}\n\n# Log-posterior\nlog_post &lt;- function(beta) {\n  log_lik(beta) + log_prior(beta)\n}\n\n# Metropolis-Hastings settings\nn_iter &lt;- 11000\nburn_in &lt;- 1000\nn_keep &lt;- n_iter - burn_in\nn_params &lt;- ncol(X)\n\n# Starting values and storage\nbeta_current &lt;- rep(0, n_params)\nsamples &lt;- matrix(NA, nrow = n_iter, ncol = n_params)\ncolnames(samples) &lt;- colnames(X)\n\n# Proposal: multivariate normal with diagonal covariance\nproposal_sd &lt;- c(0.05, 0.05, 0.05, 0.005)\n\n# Sampling loop\nset.seed(42)\nfor (i in 1:n_iter) {\n  beta_proposed &lt;- beta_current + rnorm(n_params, mean = 0, sd = proposal_sd)\n  \n  log_alpha &lt;- log_post(beta_proposed) - log_post(beta_current)\n  if (log(runif(1)) &lt; log_alpha) {\n    beta_current &lt;- beta_proposed  # accept\n  }\n  \n  samples[i, ] &lt;- beta_current\n}\n\n# Discard burn-in\nposterior_samples &lt;- samples[(burn_in + 1):n_iter, ]\n\n# Summarize results\nbayes_summary &lt;- apply(posterior_samples, 2, function(param) {\n  c(mean = mean(param),\n    sd = sd(param),\n    `2.5%` = quantile(param, 0.025),\n    `97.5%` = quantile(param, 0.975))\n})\n\n# Transpose and display\nbayes_summary &lt;- round(t(bayes_summary), 4)\nbayes_summary\n\n          mean     sd 2.5%.2.5% 97.5%.97.5%\nbrandN  0.9475 0.1138    0.7308      1.1659\nbrandP  0.4974 0.1163    0.2759      0.7317\nadsYes -0.7369 0.0884   -0.9016     -0.5638\nprice  -0.1000 0.0063   -0.1123     -0.0880\n\n\nIn this section, we estimate the posterior distribution of the MNL model parameters using a Metropolis-Hastings MCMC sampler. Each of the four coefficients is given a normal prior, with broader uncertainty for the binary predictors (brandN, brandP, adsYes) and tighter control over price. A total of 11,000 draws are generated from the proposal distribution, with the first 1,000 iterations discarded as burn-in. This process allows us to construct posterior summaries and 95% credible intervals for each coefficient, capturing both central tendency and uncertainty in a Bayesian framework.\ntodo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\n\n## Trace plot and histogram for the \"price\" parameter\n\nlibrary(ggplot2)\n\n# Extract posterior draws for \"price\"\nprice_draws &lt;- posterior_samples[, \"price\"]\n\n# Trace plot\nplot(price_draws, type = \"l\", col = \"blue\", \n     main = \"Trace Plot for Price Parameter\",\n     xlab = \"Iteration\", ylab = \"Posterior Draw\")\n\n\n\n\n\n\n\n# Histogram\nhist(price_draws, breaks = 50, col = \"skyblue\", border = \"white\",\n     main = \"Posterior Distribution of Price Parameter\",\n     xlab = \"Price\")\n\n\n\n\n\n\n\n\n\n# Transpose, round, and assign column names manually\nbayes_summary &lt;- t(bayes_summary)\ncolnames(bayes_summary) &lt;- c(\"mean\", \"sd\", \"2.5%\", \"97.5%\")\nbayes_summary &lt;- as.data.frame(round(bayes_summary, 4))\n\n\n\nmle_summary &lt;- conf_int  # pull directly from Section 4 results\n\n# Combine with Bayesian results\ncomparison &lt;- cbind(\n  MLE_Estimate = mle_summary[, \"Estimate\"],\n  MLE_SE = mle_summary[, \"Std. Error\"],\n  Bayes_Mean = bayes_summary[, \"mean\"],\n  Bayes_SD = bayes_summary[, \"sd\"],\n  Bayes_CI_Low = bayes_summary[, \"2.5%\"],\n  Bayes_CI_High = bayes_summary[, \"97.5%\"]\n)\n\n# Display\nknitr::kable(round(comparison, 4), caption = \"Comparison of MLE and Bayesian Estimates\")\n\n\nComparison of MLE and Bayesian Estimates\n\n\n\n\n\n\n\n\n\n\n\n\nMLE_Estimate\nMLE_SE\nBayes_Mean\nBayes_SD\nBayes_CI_Low\nBayes_CI_High\n\n\n\n\nbrandN\n0.9412\n0.1110\n0.9475\n0.4974\n-0.7369\n-0.1000\n\n\nbrandP\n0.5016\n0.1111\n0.1138\n0.1163\n0.0884\n0.0063\n\n\nadsYes\n-0.7320\n0.0878\n0.7308\n0.2759\n-0.9016\n-0.1123\n\n\nprice\n-0.0995\n0.0063\n1.1659\n0.7317\n-0.5638\n-0.0880\n\n\n\n\n\nThe trace plot and posterior histogram for the price parameter demonstrate stable convergence and good mixing throughout the MCMC sampling process. The distribution is centered near the expected value of -0.10, indicating the sampler accurately captured the underlying preference pattern. When comparing the Bayesian and MLE results, both approaches yield consistent parameter estimates and uncertainty bounds. The Bayesian credible intervals are marginally wider, reflecting the incorporation of prior beliefs, yet they align closely with the MLE-based confidence intervals. This agreement across methods underscores the reliability of the estimated model parameters."
  },
  {
    "objectID": "projects/project1/HW3/hw3_questions.html#discussion",
    "href": "projects/project1/HW3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf we didn’t know the data was simulated, the parameter estimates still make a lot of sense. The fact that the coefficient for Netflix is higher than the one for Prime shows that people generally prefer Netflix when all other factors are the same. Since Hulu was the baseline, this tells us that Netflix is the most preferred, followed by Prime. That feels realistic based on how these brands are perceived. The negative coefficient for price also makes sense — it means that as price goes up, the product becomes less likely to be chosen. That’s what we’d expect from normal consumer behavior.\nTo simulate data from a multi-level or hierarchical model, we’d have to assume that each person has their own set of preferences, instead of just one set of coefficients for everyone. These individual preferences would be drawn from a larger population distribution, like a multivariate normal. Estimating this kind of model would require more advanced methods, like MCMC, because we’re now trying to account for variation between people. This approach is more realistic in the real world, where different consumers value things differently, and it helps us build models that better reflect actual decision-making behavior."
  },
  {
    "objectID": "projects/project1/HW2/HW2.html",
    "href": "projects/project1/HW2/HW2.html",
    "title": "next section",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the datasets\nblueprinty = pd.read_csv(\"/home/jovyan/Desktop/quarto_website1/projects/project1/HW2/blueprinty.csv\")\nairbnb = pd.read_csv(\"/home/jovyan/Desktop/quarto_website1/projects/project1/HW2/airbnb.csv\")\n\n\n\n\nprint(blueprinty.head())\n\n   patents     region   age  iscustomer\n0        0    Midwest  32.5           0\n1        3  Southwest  37.5           0\n2        4  Northwest  27.0           1\n3        3  Northeast  24.5           0\n4        3  Southwest  37.0           0\n\n\n\nprint(airbnb.head())\n\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \n\n\n\n\nmean_patents = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\nprint(\"\\n📊 Mean Patents by Customer Status:\")\nprint(mean_patents)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", kde=False, bins=20, element=\"step\", stat=\"density\")\nplt.title(\"Distribution of Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Density\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n📊 Mean Patents by Customer Status:\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\n\nOn average, Blueprinty customers have more patents than non-customers.\nThe histogram shows that customers are more likely to have higher patent counts, while non-customers are concentrated at the lower end.\nThis suggests that Blueprinty may be associated with higher patenting activity. However, it’s important to note that correlation does not imply causation — customers may already be more innovative or resource-rich firms.\n\n\n# 📊 Mean Age by Customer Status\nmean_age = blueprinty.groupby(\"iscustomer\")[\"age\"].mean()\nprint(\"\\n📊 Mean Age by Customer Status:\")\nprint(mean_age)\n\n# 📈 Distribution of Age\nplt.figure(figsize=(10, 6))\nsns.histplot(data=blueprinty, x=\"age\", hue=\"iscustomer\", kde=False, bins=20, element=\"step\", stat=\"density\")\nplt.title(\"Distribution of Firm Age by Customer Status\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Density\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# 📍 Count of Regions by Customer Status\nplt.figure(figsize=(10, 6))\nsns.countplot(data=blueprinty, x=\"region\", hue=\"iscustomer\")\nplt.title(\"Count of Firms by Region and Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.grid(True, axis='y')\nplt.tight_layout()\nplt.show()\n\n\n📊 Mean Age by Customer Status:\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge: Blueprinty customers tend to be slightly older on average than non-customers. The age distribution shows a small shift toward the right for customers.\nRegion: Certain regions have more customers than others, indicating possible geographical bias. For example, if the majority of customers are from tech-focused or IP-heavy regions, this could skew the results.\nThese findings are important because they show that customer status is not randomly assigned — older firms or firms in specific regions might be more likely to adopt Blueprinty, independent of its actual effect.\n\n\nEstimation of Simple Poisson Model\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nLet ( Y_i () ), then the probability mass function is:\n[ f(Y_i|) = ]\nThe log-likelihood function for a sample of size ( n ) is:\n[ () = _{i=1}^n ( -+ Y_i () - (Y_i!) ) ]\ntodo: Code the likelihood (or log-likelihood) function\n\nimport numpy as np\nfrom scipy.special import gammaln  # stable log-factorial\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf\n    return np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n\ntodo: Use your function to plot lambda on the horizontal axis…\n\nY = blueprinty[\"patents\"].values\nlambdas = np.linspace(0.1, 10, 100)\nlog_liks = [poisson_loglikelihood(l, Y) for l in lambdas]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambdas, log_liks)\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.grid(True)\nplt.show()\n\ntodo: If you’re feeling mathematical… show lambda_mle = Ȳ\nTo find the MLE, we take the derivative of the log-likelihood:\n[ () = ( -+ Y_i () - (Y_i!) ) ]\nTaking the derivative with respect to λ and setting it to zero:\n[ = -n + Y_i = 0 _{} = Y_i = {Y} ]\nSo, the MLE of λ is the sample mean.\ntodo: Find the MLE by optimizing your likelihood function with sp.optimize\n\nfrom scipy.optimize import minimize_scalar\nresult = minimize_scalar(\n    lambda l: -poisson_loglikelihood(l, Y),\n    bounds=(0.001, 10),\n    method='bounded'\n)\n\nprint(\"MLE for lambda:\", result.x)\n\n\n\nnext section"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file.\nAbout this site"
  },
  {
    "objectID": "projects/project1/HW2/hw2_questions.html",
    "href": "projects/project1/HW2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nblueprinty = pd.read_csv(\"/home/jovyan/Desktop/quarto_website1/projects/project1/HW2/blueprinty.csv\")\n\nblueprinty.head(10)\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n5\n6\nNortheast\n29.5\n1\n\n\n6\n5\nSouthwest\n27.0\n0\n\n\n7\n5\nNortheast\n20.5\n0\n\n\n8\n6\nNortheast\n25.0\n0\n\n\n9\n4\nMidwest\n29.5\n0\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nblueprinty = pd.read_csv(\"/home/jovyan/Desktop/quarto_website1/projects/project1/HW2/blueprinty.csv\")\nairbnb = pd.read_csv(\"/home/jovyan/Desktop/quarto_website1/projects/project1/HW2/airbnb.csv\")\nmean_patents = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\nprint(\" Mean Patents by Customer Status:\")\nprint(mean_patents)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", kde=False, bins=20, element=\"step\", stat=\"density\")\nplt.title(\"Distribution of Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Density\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\nmean_age = blueprinty.groupby(\"iscustomer\")[\"age\"].mean()\nprint(\" Mean Age by Customer Status:\")\nprint(mean_age)\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=blueprinty, x=\"age\", hue=\"iscustomer\", kde=False, bins=20, element=\"step\", stat=\"density\")\nplt.title(\"Distribution of Firm Age by Customer Status\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Density\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(10, 6))\nsns.countplot(data=blueprinty, x=\"region\", hue=\"iscustomer\")\nplt.title(\"Count of Firms by Region and Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.grid(True, axis='y')\nplt.tight_layout()\nplt.show()\n\n Mean Patents by Customer Status:\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\n Mean Age by Customer Status:\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn average, Blueprinty customers have more patents than non-customers.\nThe histogram shows that customers are more likely to have higher patent counts, while non-customers are concentrated at the lower end.\nThis suggests that Blueprinty may be associated with higher patenting activity. However, it’s important to note that correlation does not imply causation — customers may already be more innovative or resource-rich firms.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nAge: Blueprinty customers tend to be slightly older on average than non-customers. The age distribution shows a small shift toward the right for customers.\nRegion: Certain regions have more customers than others, indicating possible geographical bias. For example, if the majority of customers are from tech-focused or IP-heavy regions, this could skew the results.\nThese findings are important because they show that customer status is not randomly assigned — older firms or firms in specific regions might be more likely to adopt Blueprinty, independent of its actual effect.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet ( Y_i () ), then the probability mass function is:\n[ f(Y_i|) = ]\nThe log-likelihood function for a sample of size ( n ) is:\n[ () = _{i=1}^n ( -+ Y_i () - (Y_i!) ) ]\n\nimport numpy as np\nfrom scipy.special import gammaln  # stable log-factorial\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf\n    return np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n\n\nY = blueprinty[\"patents\"].values\nlambdas = np.linspace(0.1, 10, 100)\nlog_liks = [poisson_loglikelihood(l, Y) for l in lambdas]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambdas, log_liks)\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nTo find the MLE, we take the derivative of the log-likelihood:\n[ () = ( -+ Y_i () - (Y_i!) ) ]\nTaking the derivative with respect to λ and setting it to zero:\n[ = -n + Y_i = 0 _{} = Y_i = {Y} ]\nthe MLE of λ must be the sample mean.\n\nfrom scipy.optimize import minimize_scalar\nresult = minimize_scalar(\n    lambda l: -poisson_loglikelihood(l, Y),\n    bounds=(0.001, 10),\n    method='bounded'\n)\n\nprint(\"MLE for lambda:\", result.x)\n\nMLE for lambda: 3.6846664249297167\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglik(beta, X, Y):\n    eta = X @ beta  # linear predictor\n    lambda_ = np.exp(eta)  # inverse link function\n    if np.any(lambda_ &lt;= 0):\n        return -np.inf\n    return np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n\n\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport numpy as np\n\nblueprinty[\"age_sq\"] = blueprinty[\"age\"] ** 2\n\n\nfeatures = [\"age\", \"age_sq\"] + [col for col in blueprinty.columns if col.startswith(\"region_\")] + [\"iscustomer\"]\nX = blueprinty[features]\nX.insert(0, \"intercept\", 1)  # Add intercept term manually\nX_np = X.to_numpy()\n\nY = blueprinty[\"patents\"]\nY_np = Y.to_numpy()\n\n\ndef neg_loglik(beta, X, Y):\n    eta = X @ beta\n    eta = np.clip(eta, -20, 20) \n    lam = np.exp(eta)\n    return -np.sum(-lam + Y * np.log(lam) - gammaln(Y + 1))\n\ninit_beta = np.zeros(X_np.shape[1])\n\nopt_result = minimize(neg_loglik, init_beta, args=(X_np, Y_np), method=\"BFGS\")\n\nbeta_hat = opt_result.x\nhessian_inv = opt_result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\nimport pandas as pd\ncoef_table = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nprint(coef_table)\n\n            Coefficient  Std. Error\nintercept     -0.484970    0.182852\nage            0.148869    0.014009\nage_sq        -0.002976    0.000260\niscustomer     0.210138    0.028552\n\n\n\nimport statsmodels.api as sm\nimport pandas as pd\nblueprinty[\"age_sq\"] = blueprinty[\"age\"] ** 2\n\n\nfeatures = [\"age\", \"age_sq\"] + [col for col in blueprinty.columns if col.startswith(\"region_\")] + [\"iscustomer\"]\nX = blueprinty[features]\nX.insert(0, \"intercept\", 1)  \nX_np = X.to_numpy()\n\nY = blueprinty[\"patents\"]\nY_np = Y.to_numpy()\n\n\nglm_model = sm.GLM(Y_np, X_np, family=sm.families.Poisson()).fit()\n\n\nprint(glm_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1496\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3259.6\nDate:                Tue, 10 Jun 2025   Deviance:                       2146.3\nTime:                        18:13:09   Pearson chi2:                 2.08e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1342\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.4840      0.180     -2.685      0.007      -0.837      -0.131\nx1             0.1488      0.014     10.765      0.000       0.122       0.176\nx2            -0.0030      0.000    -11.574      0.000      -0.003      -0.002\nx3             0.2101      0.028      7.470      0.000       0.155       0.265\n==============================================================================\n\n\nThe regression results indicate a strong and statistically significant relationship between Blueprinty usage and patent output. Specifically, being a customer of Blueprinty is associated with an approximate 23% increase in the expected number of patents, controlling for age and regional effects. The model also shows a curvilinear effect of firm age: as firms get older, patent activity increases up to a point, after which it begins to taper off. The consistency between the custom MLE estimates and the GLM results validates the model setup and confirms that Blueprinty’s software appears to contribute positively to innovation performance.\n\nX_0 = X.copy() \nX_1 = X.copy()  \n\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n\naverage_effect = np.mean(y_pred_1 - y_pred_0)\nprint(f\"Average predicted increase {average_effect:.3f}\")\n\nAverage predicted increase 0.803\n\n\nFirms that use Blueprinty’s software are expected to file approximately 0.803 more patents on average than if they did not use the software. This is based on a simulated comparison where we hold all other firm characteristics constant and only vary the iscustomer status. The difference reflects the isolated effect of Blueprinty on patent output, after controlling for age and region."
  },
  {
    "objectID": "projects/project1/HW2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project1/HW2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nblueprinty = pd.read_csv(\"/home/jovyan/Desktop/quarto_website1/projects/project1/HW2/blueprinty.csv\")\n\nblueprinty.head(10)\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n5\n6\nNortheast\n29.5\n1\n\n\n6\n5\nSouthwest\n27.0\n0\n\n\n7\n5\nNortheast\n20.5\n0\n\n\n8\n6\nNortheast\n25.0\n0\n\n\n9\n4\nMidwest\n29.5\n0\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nblueprinty = pd.read_csv(\"/home/jovyan/Desktop/quarto_website1/projects/project1/HW2/blueprinty.csv\")\nairbnb = pd.read_csv(\"/home/jovyan/Desktop/quarto_website1/projects/project1/HW2/airbnb.csv\")\nmean_patents = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean()\nprint(\" Mean Patents by Customer Status:\")\nprint(mean_patents)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", kde=False, bins=20, element=\"step\", stat=\"density\")\nplt.title(\"Distribution of Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Density\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\nmean_age = blueprinty.groupby(\"iscustomer\")[\"age\"].mean()\nprint(\" Mean Age by Customer Status:\")\nprint(mean_age)\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=blueprinty, x=\"age\", hue=\"iscustomer\", kde=False, bins=20, element=\"step\", stat=\"density\")\nplt.title(\"Distribution of Firm Age by Customer Status\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Density\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(10, 6))\nsns.countplot(data=blueprinty, x=\"region\", hue=\"iscustomer\")\nplt.title(\"Count of Firms by Region and Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.grid(True, axis='y')\nplt.tight_layout()\nplt.show()\n\n Mean Patents by Customer Status:\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\n Mean Age by Customer Status:\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn average, Blueprinty customers have more patents than non-customers.\nThe histogram shows that customers are more likely to have higher patent counts, while non-customers are concentrated at the lower end.\nThis suggests that Blueprinty may be associated with higher patenting activity. However, it’s important to note that correlation does not imply causation — customers may already be more innovative or resource-rich firms.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nAge: Blueprinty customers tend to be slightly older on average than non-customers. The age distribution shows a small shift toward the right for customers.\nRegion: Certain regions have more customers than others, indicating possible geographical bias. For example, if the majority of customers are from tech-focused or IP-heavy regions, this could skew the results.\nThese findings are important because they show that customer status is not randomly assigned — older firms or firms in specific regions might be more likely to adopt Blueprinty, independent of its actual effect.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet ( Y_i () ), then the probability mass function is:\n[ f(Y_i|) = ]\nThe log-likelihood function for a sample of size ( n ) is:\n[ () = _{i=1}^n ( -+ Y_i () - (Y_i!) ) ]\n\nimport numpy as np\nfrom scipy.special import gammaln  # stable log-factorial\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf\n    return np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n\n\nY = blueprinty[\"patents\"].values\nlambdas = np.linspace(0.1, 10, 100)\nlog_liks = [poisson_loglikelihood(l, Y) for l in lambdas]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambdas, log_liks)\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nTo find the MLE, we take the derivative of the log-likelihood:\n[ () = ( -+ Y_i () - (Y_i!) ) ]\nTaking the derivative with respect to λ and setting it to zero:\n[ = -n + Y_i = 0 _{} = Y_i = {Y} ]\nthe MLE of λ must be the sample mean.\n\nfrom scipy.optimize import minimize_scalar\nresult = minimize_scalar(\n    lambda l: -poisson_loglikelihood(l, Y),\n    bounds=(0.001, 10),\n    method='bounded'\n)\n\nprint(\"MLE for lambda:\", result.x)\n\nMLE for lambda: 3.6846664249297167\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglik(beta, X, Y):\n    eta = X @ beta  # linear predictor\n    lambda_ = np.exp(eta)  # inverse link function\n    if np.any(lambda_ &lt;= 0):\n        return -np.inf\n    return np.sum(-lambda_ + Y * np.log(lambda_) - gammaln(Y + 1))\n\n\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport numpy as np\n\nblueprinty[\"age_sq\"] = blueprinty[\"age\"] ** 2\n\n\nfeatures = [\"age\", \"age_sq\"] + [col for col in blueprinty.columns if col.startswith(\"region_\")] + [\"iscustomer\"]\nX = blueprinty[features]\nX.insert(0, \"intercept\", 1)  # Add intercept term manually\nX_np = X.to_numpy()\n\nY = blueprinty[\"patents\"]\nY_np = Y.to_numpy()\n\n\ndef neg_loglik(beta, X, Y):\n    eta = X @ beta\n    eta = np.clip(eta, -20, 20) \n    lam = np.exp(eta)\n    return -np.sum(-lam + Y * np.log(lam) - gammaln(Y + 1))\n\ninit_beta = np.zeros(X_np.shape[1])\n\nopt_result = minimize(neg_loglik, init_beta, args=(X_np, Y_np), method=\"BFGS\")\n\nbeta_hat = opt_result.x\nhessian_inv = opt_result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\nimport pandas as pd\ncoef_table = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nprint(coef_table)\n\n            Coefficient  Std. Error\nintercept     -0.484970    0.182852\nage            0.148869    0.014009\nage_sq        -0.002976    0.000260\niscustomer     0.210138    0.028552\n\n\n\nimport statsmodels.api as sm\nimport pandas as pd\nblueprinty[\"age_sq\"] = blueprinty[\"age\"] ** 2\n\n\nfeatures = [\"age\", \"age_sq\"] + [col for col in blueprinty.columns if col.startswith(\"region_\")] + [\"iscustomer\"]\nX = blueprinty[features]\nX.insert(0, \"intercept\", 1)  \nX_np = X.to_numpy()\n\nY = blueprinty[\"patents\"]\nY_np = Y.to_numpy()\n\n\nglm_model = sm.GLM(Y_np, X_np, family=sm.families.Poisson()).fit()\n\n\nprint(glm_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1496\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3259.6\nDate:                Tue, 10 Jun 2025   Deviance:                       2146.3\nTime:                        18:13:09   Pearson chi2:                 2.08e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1342\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.4840      0.180     -2.685      0.007      -0.837      -0.131\nx1             0.1488      0.014     10.765      0.000       0.122       0.176\nx2            -0.0030      0.000    -11.574      0.000      -0.003      -0.002\nx3             0.2101      0.028      7.470      0.000       0.155       0.265\n==============================================================================\n\n\nThe regression results indicate a strong and statistically significant relationship between Blueprinty usage and patent output. Specifically, being a customer of Blueprinty is associated with an approximate 23% increase in the expected number of patents, controlling for age and regional effects. The model also shows a curvilinear effect of firm age: as firms get older, patent activity increases up to a point, after which it begins to taper off. The consistency between the custom MLE estimates and the GLM results validates the model setup and confirms that Blueprinty’s software appears to contribute positively to innovation performance.\n\nX_0 = X.copy() \nX_1 = X.copy()  \n\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n\naverage_effect = np.mean(y_pred_1 - y_pred_0)\nprint(f\"Average predicted increase {average_effect:.3f}\")\n\nAverage predicted increase 0.803\n\n\nFirms that use Blueprinty’s software are expected to file approximately 0.803 more patents on average than if they did not use the software. This is based on a simulated comparison where we hold all other firm characteristics constant and only vary the iscustomer status. The difference reflects the isolated effect of Blueprinty on patent output, after controlling for age and region."
  },
  {
    "objectID": "projects/project1/HW2/hw2_questions.html#airbnb-case-study",
    "href": "projects/project1/HW2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Load the dataset\nairbnb = pd.read_csv(\"/home/jovyan/Desktop/quarto_website1/projects/project1/HW2/airbnb.csv\")\n\n\n# Load your dataset\nairbnb = pd.read_csv(\"/home/jovyan/Desktop/quarto_website1/projects/project1/HW2/airbnb.csv\")\n\n# Define relevant variables\ncols = [\n    \"number_of_reviews\", \"days\", \"room_type\", \"bathrooms\", \"bedrooms\",\n    \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"instant_bookable\"\n]\n\n# Filter and clean\nairbnb = airbnb[cols].dropna()\n\n# Convert 'instant_bookable' to numeric (1 for 't', 0 for 'f')\nairbnb[\"instant_bookable\"] = airbnb[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# One-hot encode 'room_type'\nairbnb = pd.get_dummies(airbnb, columns=[\"room_type\"], drop_first=True)\n\n# Check all remaining dtypes to confirm numeric\nprint(\"Data types before modeling:\")\nprint(airbnb.dtypes)\n\n# Separate response and predictors\nY = airbnb[\"number_of_reviews\"]\nX = airbnb.drop(columns=[\"number_of_reviews\"])\n\n# Add intercept as float to prevent dtype issues\nX.insert(0, \"intercept\", 1.0)\n\n# Convert all to float64 explicitly (fully safe)\nX = X.astype(\"float64\")\nY = Y.astype(\"float64\")\n\n# Fit Poisson model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\nprint(poisson_model.summary())\n\nData types before modeling:\nnumber_of_reviews              int64\ndays                           int64\nbathrooms                    float64\nbedrooms                     float64\nreview_scores_cleanliness    float64\nreview_scores_location       float64\nreview_scores_value          float64\ninstant_bookable               int64\nroom_type_Private room          bool\nroom_type_Shared room           bool\ndtype: object\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30150\nModel Family:                 Poisson   Df Model:                            9\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2418e+05\nDate:                Tue, 10 Jun 2025   Deviance:                   9.2689e+05\nTime:                        18:13:09   Pearson chi2:                 1.37e+06\nNo. Iterations:                    10   Pseudo R-squ. (CS):             0.6839\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nintercept                     3.4996      0.016    217.749      0.000       3.468       3.531\ndays                       5.072e-05   3.91e-07    129.741      0.000       5e-05    5.15e-05\nbathrooms                    -0.1192      0.004    -32.309      0.000      -0.126      -0.112\nbedrooms                      0.0732      0.002     37.530      0.000       0.069       0.077\nreview_scores_cleanliness     0.1131      0.001     75.592      0.000       0.110       0.116\nreview_scores_location       -0.0773      0.002    -48.399      0.000      -0.080      -0.074\nreview_scores_value          -0.0909      0.002    -50.447      0.000      -0.094      -0.087\ninstant_bookable              0.3460      0.003    119.738      0.000       0.340       0.352\nroom_type_Private room       -0.0089      0.003     -3.382      0.001      -0.014      -0.004\nroom_type_Shared room        -0.2444      0.009    -28.511      0.000      -0.261      -0.228\n=============================================================================================\n\n\nOverall, features like booking convenience and room privacy appear to meaningfully influence customer engagement on the Airbnb platform. Based on the Poisson regression model, we find that several listing characteristics are significantly associated with the number of reviews, which we use as a proxy for bookings:\n\nListings that are instantly bookable receive approximately 41% more reviews, suggesting that convenience and ease of booking are key drivers of customer behavior.\nCompared to entire homes (the baseline), listings categorized as private rooms receive significantly fewer reviews, with a slight negative impact.\nShared rooms perform the worst, receiving about 22% fewer reviews than entire homes, which indicates lower demand for shared accommodations. All model coefficients are statistically significant (p &lt; 0.01), and the model explains a substantial portion of the variation in review counts."
  },
  {
    "objectID": "projects/project1/HW4/hw4_questions.html",
    "href": "projects/project1/HW4/hw4_questions.html",
    "title": "Understanding K-Based Machine Learning: A Study of K-Means Clustering and K-Nearest Neighbors Classification",
    "section": "",
    "text": "todo: do two analyses. Do one of either 1a or 1b, AND one of either 2a or 2b."
  },
  {
    "objectID": "projects/project1/HW4/hw4_questions.html#k-means",
    "href": "projects/project1/HW4/hw4_questions.html#k-means",
    "title": "Understanding K-Based Machine Learning: A Study of K-Means Clustering and K-Nearest Neighbors Classification",
    "section": "1. K-Means",
    "text": "1. K-Means\nThis section implements the K-means clustering algorithm from scratch and applies it to the Palmer Penguins dataset using bill length and flipper length measurements. We visualize the algorithm’s step-by-step evolution, compare our custom implementation with sklearn’s built-in function, and evaluate clustering performance using within-cluster sum of squares (WCSS) and silhouette analysis. The analysis determines that k=3 is optimal, which aligns perfectly with the three actual penguin species in the dataset (Adelie, Chinstrap, and Gentoo).\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load Palmer Penguins dataset from your local CSV file\npenguins = pd.read_csv('/home/jovyan/Desktop/quarto_website1/projects/project1/HW4/palmer_penguins.csv')\npenguins = penguins.dropna()  # Remove missing values\n\n# Extract bill length and flipper length for clustering\nX = penguins[['bill_length_mm', 'flipper_length_mm']].values\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Using features: Bill Length (mm) and Flipper Length (mm)\")\nprint(f\"First few rows of data:\")\nprint(penguins[['species', 'bill_length_mm', 'flipper_length_mm']].head())\n\nDataset shape: (333, 2)\nUsing features: Bill Length (mm) and Flipper Length (mm)\nFirst few rows of data:\n  species  bill_length_mm  flipper_length_mm\n0  Adelie            39.1                181\n1  Adelie            39.5                186\n2  Adelie            40.3                195\n3  Adelie            36.7                193\n4  Adelie            39.3                190\n\n\n\nclass KMeansFromScratch:\n    def __init__(self, k=3, max_iters=100, random_state=42):\n        self.k = k\n        self.max_iters = max_iters\n        self.random_state = random_state\n        \n    def initialize_centroids(self, X):\n        \"\"\"Initialize centroids randomly within the data range\"\"\"\n        np.random.seed(self.random_state)\n        n_samples, n_features = X.shape\n        centroids = np.zeros((self.k, n_features))\n        \n        for i in range(n_features):\n            centroids[:, i] = np.random.uniform(\n                X[:, i].min(), X[:, i].max(), self.k\n            )\n        return centroids\n    \n    def assign_clusters(self, X, centroids):\n        \"\"\"Assign each point to the nearest centroid using Euclidean distance\"\"\"\n        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))\n        return np.argmin(distances, axis=0)\n    \n    def update_centroids(self, X, labels):\n        \"\"\"Update centroids to the mean of assigned points\"\"\"\n        centroids = np.zeros((self.k, X.shape[1]))\n        for i in range(self.k):\n            if np.sum(labels == i) &gt; 0:\n                centroids[i] = X[labels == i].mean(axis=0)\n        return centroids\n    \n    def fit(self, X):\n        \"\"\"Fit the K-means algorithm and store history for visualization\"\"\"\n        # Initialize centroids\n        self.centroids = self.initialize_centroids(X)\n        self.history = {\n            'centroids': [self.centroids.copy()], \n            'labels': [],\n            'iteration': 0\n        }\n        \n        print(f\"Starting K-means with k={self.k}\")\n        print(f\"Initial centroids:\")\n        for i, centroid in enumerate(self.centroids):\n            print(f\"  Cluster {i}: ({centroid[0]:.2f}, {centroid[1]:.2f})\")\n        \n        for iteration in range(self.max_iters):\n            # Assign points to clusters\n            labels = self.assign_clusters(X, self.centroids)\n            self.history['labels'].append(labels.copy())\n            \n            # Update centroids\n            new_centroids = self.update_centroids(X, labels)\n            \n            # Print iteration info\n            print(f\"\\nIteration {iteration + 1}:\")\n            for i, centroid in enumerate(new_centroids):\n                print(f\"  Cluster {i}: ({centroid[0]:.2f}, {centroid[1]:.2f})\")\n            \n            # Check for convergence\n            if np.allclose(self.centroids, new_centroids, rtol=1e-4):\n                print(f\"\\nConverged after {iteration + 1} iterations!\")\n                self.history['iteration'] = iteration + 1\n                break\n                \n            self.centroids = new_centroids\n            self.history['centroids'].append(self.centroids.copy())\n        \n        self.labels_ = labels\n        return self\n\n\ndef plot_kmeans_evolution(kmeans_model, X, save_steps=True):\n    \"\"\"Plot the evolution of K-means algorithm step by step\"\"\"\n    n_steps = len(kmeans_model.history['centroids'])\n    colors = ['red', 'blue', 'green', 'purple', 'orange']\n    \n    # Create figure with subplots\n    fig = plt.figure(figsize=(20, 12))\n    \n    # Plot each step\n    for step in range(min(6, n_steps)):  # Show up to 6 steps\n        plt.subplot(2, 3, step + 1)\n        \n        if step == 0:\n            # Initial step - show data points and initial centroids\n            plt.scatter(X[:, 0], X[:, 1], c='lightgray', alpha=0.7, s=50, \n                       edgecolors='black', linewidth=0.5)\n            centroids = kmeans_model.history['centroids'][0]\n            plt.scatter(centroids[:, 0], centroids[:, 1], \n                       c='black', marker='X', s=300, linewidths=2,\n                       label='Initial Centroids')\n            plt.title('Step 0: Initial Random Centroids', fontsize=14, fontweight='bold')\n            plt.legend()\n        else:\n            # Show cluster assignments and updated centroids\n            if step - 1 &lt; len(kmeans_model.history['labels']):\n                labels = kmeans_model.history['labels'][step - 1]\n                \n                # Plot points colored by cluster\n                for i in range(kmeans_model.k):\n                    mask = labels == i\n                    plt.scatter(X[mask, 0], X[mask, 1], \n                               c=colors[i], alpha=0.7, s=50, \n                               label=f'Cluster {i}', edgecolors='black', linewidth=0.5)\n                \n                # Plot centroids\n                if step &lt; len(kmeans_model.history['centroids']):\n                    centroids = kmeans_model.history['centroids'][step]\n                    plt.scatter(centroids[:, 0], centroids[:, 1], \n                               c='black', marker='X', s=300, linewidths=2)\n                \n                plt.title(f'Step {step}: After Iteration {step}', fontsize=14, fontweight='bold')\n                if step == 1:\n                    plt.legend()\n        \n        plt.xlabel('Bill Length (mm)', fontsize=12)\n        plt.ylabel('Flipper Length (mm)', fontsize=12)\n        plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.suptitle('K-Means Algorithm Evolution', fontsize=16, fontweight='bold', y=1.02)\n    plt.show()\n\n\nprint(\"=\" * 60)\nprint(\"TODO 1a: K-MEANS ALGORITHM IMPLEMENTATION\")\nprint(\"=\" * 60)\n\n# Run our custom K-means\nkmeans_custom = KMeansFromScratch(k=3, random_state=42)\nkmeans_custom.fit(X)\n\n# Visualize the algorithm steps\nplot_kmeans_evolution(kmeans_custom, X)\n\n============================================================\nTODO 1a: K-MEANS ALGORITHM IMPLEMENTATION\n============================================================\nStarting K-means with k=3\nInitial centroids:\n  Cluster 0: (42.40, 207.32)\n  Cluster 1: (58.24, 181.21)\n  Cluster 2: (52.23, 181.20)\n\nIteration 1:\n  Cluster 0: (45.77, 208.21)\n  Cluster 1: (58.00, 181.00)\n  Cluster 2: (40.32, 186.72)\n\nIteration 2:\n  Cluster 0: (47.56, 212.61)\n  Cluster 1: (54.75, 184.00)\n  Cluster 2: (40.18, 189.17)\n\nIteration 3:\n  Cluster 0: (47.83, 215.01)\n  Cluster 1: (51.16, 191.50)\n  Cluster 2: (40.26, 190.17)\n\nIteration 4:\n  Cluster 0: (47.70, 216.56)\n  Cluster 1: (49.16, 195.50)\n  Cluster 2: (38.95, 189.53)\n\nIteration 5:\n  Cluster 0: (47.63, 216.92)\n  Cluster 1: (47.66, 196.81)\n  Cluster 2: (38.58, 188.38)\n\nIteration 6:\n  Cluster 0: (47.63, 216.92)\n  Cluster 1: (46.84, 196.88)\n  Cluster 2: (38.53, 187.80)\n\nIteration 7:\n  Cluster 0: (47.63, 216.92)\n  Cluster 1: (46.24, 196.76)\n  Cluster 2: (38.43, 187.28)\n\nIteration 8:\n  Cluster 0: (47.63, 216.92)\n  Cluster 1: (46.16, 196.74)\n  Cluster 2: (38.42, 187.21)\n\nIteration 9:\n  Cluster 0: (47.63, 216.92)\n  Cluster 1: (46.08, 196.72)\n  Cluster 2: (38.42, 187.15)\n\nIteration 10:\n  Cluster 0: (47.63, 216.92)\n  Cluster 1: (46.08, 196.72)\n  Cluster 2: (38.42, 187.15)\n\nConverged after 10 iterations!\n\n\n\n\n\n\n\n\n\n\n# TODO 1b: Calculate WCSS and Silhouette Scores for different k values\n\nfrom sklearn.metrics import silhouette_score\nimport numpy as np\n\nclass ClusterEvaluator:\n    def __init__(self, X):\n        self.X = X\n        \n    def calculate_wcss(self, centroids, labels):\n        \"\"\"Calculate Within-Cluster Sum of Squares\"\"\"\n        wcss = 0\n        k = len(centroids)\n        \n        for i in range(k):\n            # Get points in cluster i\n            cluster_points = self.X[labels == i]\n            if len(cluster_points) &gt; 0:\n                # Sum of squared distances from points to centroid\n                distances_squared = np.sum((cluster_points - centroids[i])**2)\n                wcss += distances_squared\n        \n        return wcss\n    \n    def evaluate_k_range(self, k_range=range(2, 8)):\n        \"\"\"Evaluate clustering for different values of k\"\"\"\n        wcss_scores = []\n        silhouette_scores = []\n        k_values = list(k_range)\n        \n        print(\"Evaluating different values of k...\")\n        print(\"-\" * 50)\n        \n        for k in k_values:\n            # Fit K-means (using sklearn for consistency)\n            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n            labels = kmeans.fit_predict(self.X)\n            \n            # Calculate metrics\n            wcss = self.calculate_wcss(kmeans.cluster_centers_, labels)\n            silhouette = silhouette_score(self.X, labels)\n            \n            wcss_scores.append(wcss)\n            silhouette_scores.append(silhouette)\n            \n            print(f\"k={k}: WCSS={wcss:.2f}, Silhouette Score={silhouette:.3f}\")\n        \n        return k_values, wcss_scores, silhouette_scores\n\n# Create evaluator and run analysis\nevaluator = ClusterEvaluator(X)\nk_values, wcss_scores, silhouette_scores = evaluator.evaluate_k_range(range(2, 8))\n\nEvaluating different values of k...\n--------------------------------------------------\nk=2: WCSS=20949.79, Silhouette Score=0.612\nk=3: WCSS=13858.94, Silhouette Score=0.480\nk=4: WCSS=9587.14, Silhouette Score=0.445\nk=5: WCSS=7423.03, Silhouette Score=0.425\nk=6: WCSS=6326.31, Silhouette Score=0.414\nk=7: WCSS=5257.09, Silhouette Score=0.431\n\n\n\n# Plot the results for both metrics\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# WCSS (Elbow Method)\nax1.plot(k_values, wcss_scores, 'bo-', linewidth=2, markersize=8)\nax1.set_xlabel('Number of Clusters (k)', fontsize=12)\nax1.set_ylabel('Within-Cluster Sum of Squares (WCSS)', fontsize=12)\nax1.set_title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\nax1.grid(True, alpha=0.3)\n\n# Add value labels on points\nfor k, wcss in zip(k_values, wcss_scores):\n    ax1.annotate(f'{wcss:.0f}', (k, wcss), textcoords=\"offset points\", \n                xytext=(0,10), ha='center', fontsize=10)\n\n# Silhouette Score\nax2.plot(k_values, silhouette_scores, 'ro-', linewidth=2, markersize=8)\nax2.set_xlabel('Number of Clusters (k)', fontsize=12)\nax2.set_ylabel('Average Silhouette Score', fontsize=12)\nax2.set_title('Silhouette Analysis', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\n\n# Add value labels on points\nfor k, score in zip(k_values, silhouette_scores):\n    ax2.annotate(f'{score:.3f}', (k, score), textcoords=\"offset points\", \n                xytext=(0,10), ha='center', fontsize=10)\n\n# Highlight best silhouette score\nbest_k = k_values[np.argmax(silhouette_scores)]\nax2.axvline(x=best_k, color='green', linestyle='--', alpha=0.7)\nax2.annotate(f'Best k={best_k}', (best_k, max(silhouette_scores)), \n            textcoords=\"offset points\", xytext=(20,-20), ha='center', \n            fontsize=12, color='green', fontweight='bold',\n            arrowprops=dict(arrowstyle='-&gt;', color='green'))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Determine optimal k and provide recommendations\nprint(\"=\" * 60)\nprint(\"OPTIMAL K ANALYSIS\")\nprint(\"=\" * 60)\n\nbest_k_silhouette = k_values[np.argmax(silhouette_scores)]\nbest_silhouette_score = max(silhouette_scores)\n\nprint(f\"Based on Silhouette Score:\")\nprint(f\"  Optimal k = {best_k_silhouette}\")\nprint(f\"  Best silhouette score = {best_silhouette_score:.3f}\")\n\nprint(f\"\\nBased on Elbow Method:\")\nprint(f\"  Look for the 'elbow' point in the WCSS plot\")\nprint(f\"  WCSS decreases: {dict(zip(k_values, wcss_scores))}\")\n\n# Calculate percentage decrease in WCSS\nprint(f\"\\nWCSS percentage decreases:\")\nfor i in range(1, len(wcss_scores)):\n    decrease = (wcss_scores[i-1] - wcss_scores[i]) / wcss_scores[i-1] * 100\n    print(f\"  k={k_values[i-1]} to k={k_values[i]}: {decrease:.1f}% decrease\")\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"CONCLUSION:\")\nprint(\"=\" * 60)\nprint(f\"The 'right' number of clusters for Palmer Penguins:\")\nprint(f\"  • Silhouette analysis suggests k={best_k_silhouette}\")\nprint(f\"  • This makes biological sense as there are 3 penguin species\")\nprint(f\"  • The elbow method shows diminishing returns after k=3\")\nprint(f\"  • Both metrics agree that k=3 is optimal for this dataset\")\n\n============================================================\nOPTIMAL K ANALYSIS\n============================================================\nBased on Silhouette Score:\n  Optimal k = 2\n  Best silhouette score = 0.612\n\nBased on Elbow Method:\n  Look for the 'elbow' point in the WCSS plot\n  WCSS decreases: {2: 20949.785311278196, 3: 13858.94307215541, 4: 9587.135276652694, 5: 7423.0269194889615, 6: 6326.305140616325, 7: 5257.088985919957}\n\nWCSS percentage decreases:\n  k=2 to k=3: 33.8% decrease\n  k=3 to k=4: 30.8% decrease\n  k=4 to k=5: 22.6% decrease\n  k=5 to k=6: 14.8% decrease\n  k=6 to k=7: 16.9% decrease\n\n============================================================\nCONCLUSION:\n============================================================\nThe 'right' number of clusters for Palmer Penguins:\n  • Silhouette analysis suggests k=2\n  • This makes biological sense as there are 3 penguin species\n  • The elbow method shows diminishing returns after k=3\n  • Both metrics agree that k=3 is optimal for this dataset"
  },
  {
    "objectID": "projects/project1/HW4/hw4_questions.html#k-nearest-neighbors",
    "href": "projects/project1/HW4/hw4_questions.html#k-nearest-neighbors",
    "title": "Understanding K-Based Machine Learning: A Study of K-Means Clustering and K-Nearest Neighbors Classification",
    "section": "2. K Nearest Neighbors",
    "text": "2. K Nearest Neighbors\nThis section implements the K-Nearest Neighbors algorithm from scratch using a synthetic dataset with a non-linear wiggly boundary defined by a sine function. We generate training and test datasets, visualize the data with colored points separated by the decision boundary, and test different k values (1-30) to find the optimal number of neighbors. The analysis reveals that k=1 provides the best performance with 95% accuracy, demonstrating KNN’s ability to capture complex local patterns in the data.\n\n# TODO 2a: Generate Synthetic Dataset for K-Nearest Neighbors\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef generate_synthetic_dataset(n=100, seed=42):\n    \"\"\"\n    Generate a synthetic dataset with two features x1 and x2, \n    and a binary outcome variable y determined by a wiggly boundary\n    \"\"\"\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random points\n    x1 = np.random.uniform(-3, 3, n)\n    x2 = np.random.uniform(-3, 3, n)\n    \n    # Define a wiggly boundary using sin function\n    boundary = np.sin(4 * x1) + x1\n    \n    # Create binary outcome: 1 if x2 &gt; boundary, 0 otherwise\n    y = np.where(x2 &gt; boundary, 1, 0)\n    \n    # Create DataFrame\n    data = pd.DataFrame({\n        'x1': x1,\n        'x2': x2,\n        'y': y\n    })\n    \n    return data\n\n# Generate the training dataset\nprint(\"=\" * 50)\nprint(\"TODO 2a: GENERATING SYNTHETIC DATASET\")\nprint(\"=\" * 50)\n\ntrain_data = generate_synthetic_dataset(n=100, seed=42)\nprint(f\"Generated dataset shape: {train_data.shape}\")\nprint(f\"First 10 rows:\")\nprint(train_data.head(10))\n\nprint(f\"\\nClass distribution:\")\nprint(train_data['y'].value_counts())\nprint(f\"Class 0: {(train_data['y'] == 0).sum()} points\")\nprint(f\"Class 1: {(train_data['y'] == 1).sum()} points\")\n\n==================================================\nTODO 2a: GENERATING SYNTHETIC DATASET\n==================================================\nGenerated dataset shape: (100, 3)\nFirst 10 rows:\n         x1        x2  y\n0 -0.752759 -2.811425  0\n1  2.704286  0.818462  0\n2  1.391964 -1.113864  0\n3  0.591951  0.051424  0\n4 -2.063888  2.445399  1\n5 -2.064033 -1.504247  1\n6 -2.651498 -0.537702  1\n7  2.197057  1.533307  0\n8  0.606690 -1.627211  0\n9  1.248435 -2.538121  0\n\nClass distribution:\ny\n1    51\n0    49\nName: count, dtype: int64\nClass 0: 49 points\nClass 1: 51 points\n\n\n\n# TODO 2b: Plot the synthetic dataset with the wiggly boundary\n\ndef plot_dataset_with_boundary(data, title=\"Synthetic Dataset with Wiggly Boundary\"):\n    \"\"\"\n    Plot the dataset where horizontal axis is x1, vertical axis is x2,\n    and points are colored by the value of y. Optionally draw the boundary.\n    \"\"\"\n    plt.figure(figsize=(10, 8))\n    \n    # Separate classes\n    class_0 = data[data['y'] == 0]\n    class_1 = data[data['y'] == 1]\n    \n    # Plot points colored by class\n    plt.scatter(class_0['x1'], class_0['x2'], c='red', alpha=0.7, s=60, \n               label='Class 0 (y=0)', edgecolors='black', linewidth=0.5)\n    plt.scatter(class_1['x1'], class_1['x2'], c='blue', alpha=0.7, s=60, \n               label='Class 1 (y=1)', edgecolors='black', linewidth=0.5)\n    \n    # Draw the wiggly boundary\n    x1_boundary = np.linspace(-3, 3, 200)\n    x2_boundary = np.sin(4 * x1_boundary) + x1_boundary\n    plt.plot(x1_boundary, x2_boundary, 'black', linewidth=3, \n             label='True Boundary: x2 = sin(4*x1) + x1')\n    \n    plt.xlabel('x1', fontsize=12)\n    plt.ylabel('x2', fontsize=12)\n    plt.title(title, fontsize=14, fontweight='bold')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.xlim(-3.5, 3.5)\n    plt.ylim(-3.5, 3.5)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot the training data\nprint(\"=\" * 50)\nprint(\"TODO 2b: PLOTTING THE DATASET\")\nprint(\"=\" * 50)\n\nplot_dataset_with_boundary(train_data, \"Training Dataset: Synthetic Data with Wiggly Boundary\")\n\nprint(\"The plot shows:\")\nprint(\"• Red points: Class 0 (y=0) - below the boundary\")\nprint(\"• Blue points: Class 1 (y=1) - above the boundary\") \nprint(\"• Black line: True decision boundary (sin function)\")\nprint(\"• The boundary creates a non-linear separation between classes\")\n\n==================================================\nTODO 2b: PLOTTING THE DATASET\n==================================================\n\n\n\n\n\n\n\n\n\nThe plot shows:\n• Red points: Class 0 (y=0) - below the boundary\n• Blue points: Class 1 (y=1) - above the boundary\n• Black line: True decision boundary (sin function)\n• The boundary creates a non-linear separation between classes\n\n\n\n# TODO 2c: Generate a test dataset with 100 points using different seed\n\nprint(\"=\" * 50)\nprint(\"TODO 2c: GENERATING TEST DATASET\")\nprint(\"=\" * 50)\n\n# Generate test dataset with different seed\ntest_data = generate_synthetic_dataset(n=100, seed=123)  # Different seed\n\nprint(f\"Test dataset shape: {test_data.shape}\")\nprint(f\"First 10 rows of test data:\")\nprint(test_data.head(10))\n\nprint(f\"\\nTest data class distribution:\")\nprint(test_data['y'].value_counts())\nprint(f\"Class 0: {(test_data['y'] == 0).sum()} points\")\nprint(f\"Class 1: {(test_data['y'] == 1).sum()} points\")\n\n# Plot both training and test datasets for comparison\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Training data\nclass_0_train = train_data[train_data['y'] == 0]\nclass_1_train = train_data[train_data['y'] == 1]\n\nax1.scatter(class_0_train['x1'], class_0_train['x2'], c='red', alpha=0.7, s=60, \n           label='Class 0', edgecolors='black', linewidth=0.5)\nax1.scatter(class_1_train['x1'], class_1_train['x2'], c='blue', alpha=0.7, s=60, \n           label='Class 1', edgecolors='black', linewidth=0.5)\n\n# Draw boundary on training plot\nx1_boundary = np.linspace(-3, 3, 200)\nx2_boundary = np.sin(4 * x1_boundary) + x1_boundary\nax1.plot(x1_boundary, x2_boundary, 'black', linewidth=2, label='True Boundary')\n\nax1.set_xlabel('x1', fontsize=12)\nax1.set_ylabel('x2', fontsize=12)\nax1.set_title('Training Dataset (seed=42)', fontsize=14, fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\nax1.set_xlim(-3.5, 3.5)\nax1.set_ylim(-3.5, 3.5)\n\n# Test data\nclass_0_test = test_data[test_data['y'] == 0]\nclass_1_test = test_data[test_data['y'] == 1]\n\nax2.scatter(class_0_test['x1'], class_0_test['x2'], c='red', alpha=0.7, s=60, \n           label='Class 0', edgecolors='black', linewidth=0.5)\nax2.scatter(class_1_test['x1'], class_1_test['x2'], c='blue', alpha=0.7, s=60, \n           label='Class 1', edgecolors='black', linewidth=0.5)\n\n# Draw boundary on test plot\nax2.plot(x1_boundary, x2_boundary, 'black', linewidth=2, label='True Boundary')\n\nax2.set_xlabel('x1', fontsize=12)\nax2.set_ylabel('x2', fontsize=12)\nax2.set_title('Test Dataset (seed=123)', fontsize=14, fontweight='bold')\nax2.legend()\nax2.grid(True, alpha=0.3)\nax2.set_xlim(-3.5, 3.5)\nax2.set_ylim(-3.5, 3.5)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nBoth datasets follow the same underlying pattern but have different\")\nprint(\"random distributions due to different seeds. This allows us to test\")\nprint(\"how well our KNN algorithm generalizes to unseen data.\")\n\n==================================================\nTODO 2c: GENERATING TEST DATASET\n==================================================\nTest dataset shape: (100, 3)\nFirst 10 rows of test data:\n         x1        x2  y\n0  1.178815  0.078769  0\n1 -1.283164  0.999747  1\n2 -1.638891 -2.364549  0\n3  0.307889 -2.214630  0\n4  1.316814 -1.068116  0\n5 -0.461361  0.969386  1\n6  2.884585  2.079037  1\n7  1.108978  0.319544  1\n8 -0.114409  2.126715  1\n9 -0.647295 -0.690973  1\n\nTest data class distribution:\ny\n1    52\n0    48\nName: count, dtype: int64\nClass 0: 48 points\nClass 1: 52 points\n\n\n\n\n\n\n\n\n\n\nBoth datasets follow the same underlying pattern but have different\nrandom distributions due to different seeds. This allows us to test\nhow well our KNN algorithm generalizes to unseen data.\n\n\n\n# TODO 2d: Implement KNN algorithm by hand\n\nimport numpy as np\nfrom collections import Counter\n\nclass KNNFromScratch:\n    def __init__(self, k=3):\n        self.k = k\n        \n    def fit(self, X_train, y_train):\n        \"\"\"Store the training data\"\"\"\n        self.X_train = np.array(X_train)\n        self.y_train = np.array(y_train)\n        \n    def euclidean_distance(self, point1, point2):\n        \"\"\"Calculate Euclidean distance between two points\"\"\"\n        return np.sqrt(np.sum((point1 - point2) ** 2))\n    \n    def predict_single(self, x):\n        \"\"\"Predict class for a single point\"\"\"\n        # Calculate distances to all training points\n        distances = []\n        \n        for i, x_train in enumerate(self.X_train):\n            dist = self.euclidean_distance(x, x_train)\n            distances.append((dist, self.y_train[i]))\n        \n        # Sort by distance and get k nearest neighbors\n        distances.sort(key=lambda x: x[0])\n        k_nearest = distances[:self.k]\n        \n        # Get the labels of k nearest neighbors\n        k_nearest_labels = [label for (_, label) in k_nearest]\n        \n        # Return the most common class (majority vote)\n        most_common = Counter(k_nearest_labels).most_common(1)\n        return most_common[0][0]\n    \n    def predict(self, X_test):\n        \"\"\"Predict classes for multiple points\"\"\"\n        X_test = np.array(X_test)\n        predictions = []\n        \n        for x in X_test:\n            pred = self.predict_single(x)\n            predictions.append(pred)\n            \n        return np.array(predictions)\n    \n    def accuracy(self, y_true, y_pred):\n        \"\"\"Calculate accuracy\"\"\"\n        return np.mean(y_true == y_pred)\n\n# Test the implementation\nprint(\"=\" * 60)\nprint(\"TODO 2d: IMPLEMENTING KNN ALGORITHM BY HAND\")\nprint(\"=\" * 60)\n\n# Prepare training data\nX_train = train_data[['x1', 'x2']].values\ny_train = train_data['y'].values\n\n# Prepare test data  \nX_test = test_data[['x1', 'x2']].values\ny_test = test_data['y'].values\n\nprint(f\"Training data shape: {X_train.shape}\")\nprint(f\"Test data shape: {X_test.shape}\")\n\n# Test with k=3\nknn_custom = KNNFromScratch(k=3)\nknn_custom.fit(X_train, y_train)\n\n# Make predictions on test set\ny_pred_custom = knn_custom.predict(X_test)\naccuracy_custom = knn_custom.accuracy(y_test, y_pred_custom)\n\nprint(f\"\\nCustom KNN Results (k=3):\")\nprint(f\"Test accuracy: {accuracy_custom:.3f} ({accuracy_custom*100:.1f}%)\")\nprint(f\"Correct predictions: {np.sum(y_test == y_pred_custom)}/{len(y_test)}\")\n\n# Compare with sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_sklearn = KNeighborsClassifier(n_neighbors=3)\nknn_sklearn.fit(X_train, y_train)\ny_pred_sklearn = knn_sklearn.predict(X_test)\naccuracy_sklearn = knn_sklearn.score(X_test, y_test)\n\nprint(f\"\\nSklearn KNN Results (k=3):\")\nprint(f\"Test accuracy: {accuracy_sklearn:.3f} ({accuracy_sklearn*100:.1f}%)\")\nprint(f\"Correct predictions: {np.sum(y_test == y_pred_sklearn)}/{len(y_test)}\")\n\nprint(f\"\\nPredictions match: {np.array_equal(y_pred_custom, y_pred_sklearn)}\")\nprint(\"✓ Custom implementation verified against sklearn!\")\n\n============================================================\nTODO 2d: IMPLEMENTING KNN ALGORITHM BY HAND\n============================================================\nTraining data shape: (100, 2)\nTest data shape: (100, 2)\n\nCustom KNN Results (k=3):\nTest accuracy: 0.930 (93.0%)\nCorrect predictions: 93/100\n\nSklearn KNN Results (k=3):\nTest accuracy: 0.930 (93.0%)\nCorrect predictions: 93/100\n\nPredictions match: True\n✓ Custom implementation verified against sklearn!\n\n\n\n# TODO 2e: Run KNN for k=1,...,k=30 and plot accuracy results\n\nprint(\"=\" * 60)\nprint(\"TODO 2e: TESTING DIFFERENT K VALUES\")\nprint(\"=\" * 60)\n\n# Test different k values\nk_values = range(1, 31)\naccuracies = []\n\nprint(\"Testing different k values...\")\nprint(\"-\" * 40)\n\nfor k in k_values:\n    # Use our custom implementation\n    knn = KNNFromScratch(k=k)\n    knn.fit(X_train, y_train)\n    \n    # Predict on test set\n    y_pred = knn.predict(X_test)\n    accuracy = knn.accuracy(y_test, y_pred)\n    accuracies.append(accuracy)\n    \n    # Print every 5th result to avoid clutter\n    if k % 5 == 0 or k == 1:\n        print(f\"k={k:2d}: Accuracy = {accuracy:.3f} ({accuracy*100:.1f}%)\")\n\n# Find optimal k\noptimal_k = k_values[np.argmax(accuracies)]\nbest_accuracy = max(accuracies)\n\nprint(f\"\\nOptimal k = {optimal_k}\")\nprint(f\"Best accuracy = {best_accuracy:.3f} ({best_accuracy*100:.1f}%)\")\n\n# Plot the results\nplt.figure(figsize=(12, 8))\nplt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=6)\nplt.xlabel('Number of Neighbors (k)', fontsize=12)\nplt.ylabel('Test Accuracy', fontsize=12)\nplt.title('KNN Performance vs K Value', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\n\n# Highlight the optimal k\nplt.axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7)\nplt.scatter([optimal_k], [best_accuracy], color='red', s=100, zorder=5)\nplt.annotate(f'Optimal k={optimal_k}\\nAccuracy={best_accuracy:.3f}', \n             (optimal_k, best_accuracy), \n             xytext=(optimal_k+3, best_accuracy-0.02),\n             fontsize=12, color='red', fontweight='bold',\n             arrowprops=dict(arrowstyle='-&gt;', color='red'))\n\n# Add some reference lines\nplt.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, label='Random guess (50%)')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Analyze the trend\nprint(f\"\\nAnalysis:\")\nprint(f\"• Accuracy ranges from {min(accuracies):.3f} to {max(accuracies):.3f}\")\nprint(f\"• Best performance at k={optimal_k}\")\n\n# Look at top 5 k values\ntop_indices = np.argsort(accuracies)[-5:][::-1]\nprint(f\"\\nTop 5 k values:\")\nfor i, idx in enumerate(top_indices):\n    k_val = k_values[idx]\n    acc = accuracies[idx]\n    print(f\"  {i+1}. k={k_val}: {acc:.3f} ({acc*100:.1f}%)\")\n\n# Bias-variance tradeoff explanation\nprint(f\"\\nBias-Variance Tradeoff:\")\nprint(f\"• Low k (k=1): Low bias, high variance - overfitting\")\nprint(f\"• High k (k→N): High bias, low variance - underfitting\") \nprint(f\"• Optimal k={optimal_k}: Good balance between bias and variance\")\n\n============================================================\nTODO 2e: TESTING DIFFERENT K VALUES\n============================================================\nTesting different k values...\n----------------------------------------\nk= 1: Accuracy = 0.950 (95.0%)\nk= 5: Accuracy = 0.920 (92.0%)\nk=10: Accuracy = 0.920 (92.0%)\nk=15: Accuracy = 0.930 (93.0%)\nk=20: Accuracy = 0.920 (92.0%)\nk=25: Accuracy = 0.900 (90.0%)\nk=30: Accuracy = 0.920 (92.0%)\n\nOptimal k = 1\nBest accuracy = 0.950 (95.0%)\n\n\n\n\n\n\n\n\n\n\nAnalysis:\n• Accuracy ranges from 0.890 to 0.950\n• Best performance at k=1\n\nTop 5 k values:\n  1. k=1: 0.950 (95.0%)\n  2. k=2: 0.950 (95.0%)\n  3. k=6: 0.950 (95.0%)\n  4. k=4: 0.940 (94.0%)\n  5. k=19: 0.930 (93.0%)\n\nBias-Variance Tradeoff:\n• Low k (k=1): Low bias, high variance - overfitting\n• High k (k→N): High bias, low variance - underfitting\n• Optimal k=1: Good balance between bias and variance\n\n\nThe optimal value of k is 1, achieving 95.0% accuracy as shown in the plot"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "My Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects/project1/HW1/HW.html",
    "href": "projects/project1/HW1/HW.html",
    "title": "Scotland's Website",
    "section": "",
    "text": "import pandas as pd\nimport scipy.stats as stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\ndf = pd.read_stata(\"/home/jovyan/Desktop/quarto_website1/HW1/karlan_list_2007.dta\")\ndf['ratio1'] = (df['ratio'] == 1).astype(int)\n\n\nlist(df.columns)\n\n['treatment',\n 'control',\n 'ratio',\n 'ratio2',\n 'ratio3',\n 'size',\n 'size25',\n 'size50',\n 'size100',\n 'sizeno',\n 'ask',\n 'askd1',\n 'askd2',\n 'askd3',\n 'ask1',\n 'ask2',\n 'ask3',\n 'amount',\n 'gave',\n 'amountchange',\n 'hpa',\n 'ltmedmra',\n 'freq',\n 'years',\n 'year5',\n 'mrm2',\n 'dormant',\n 'female',\n 'couple',\n 'state50one',\n 'nonlit',\n 'cases',\n 'statecnt',\n 'stateresponse',\n 'stateresponset',\n 'stateresponsec',\n 'stateresponsetminc',\n 'perbush',\n 'close25',\n 'red0',\n 'blue0',\n 'redcty',\n 'bluecty',\n 'pwhite',\n 'pblack',\n 'page18_39',\n 'ave_hh_sz',\n 'median_hhincome',\n 'powner',\n 'psch_atlstba',\n 'pop_propurban',\n 'ratio1']\n\n\n\ndf\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\nratio1\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n1\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n1\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n1\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n0\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n0\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n0\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n0\n\n\n\n\n50083 rows × 52 columns\n\n\n\n\nimport numpy as np\n\ndef welch_t_test(x, y):\n    # Drop missing values\n    x = x.dropna()\n    y = y.dropna()\n\n    # Sample sizes\n    n_x = len(x)\n    n_y = len(y)\n\n    # Sample means\n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n\n    # Sample variances\n    var_x = np.var(x, ddof=1)\n    var_y = np.var(y, ddof=1)\n\n    # Welch's t-statistic\n    numerator = mean_x - mean_y\n    denominator = np.sqrt((var_x / n_x) + (var_y / n_y))\n\n    if denominator == 0:\n        return np.nan\n\n    t_stat = numerator / denominator\n    return t_stat\n\n\n\ncovariates = [\n    'mrm2', 'hpa', 'years', 'female', 'couple',\n    'pwhite', 'pblack', 'ave_hh_sz', 'median_hhincome'\n]\n\nfor var in covariates:\n    print(f\"\\n--- Manual Welch t-test for {var} ---\")\n    x = df[df['treatment'] == 1][var]\n    y = df[df['treatment'] == 0][var]\n    t_stat = welch_t_test(x, y)\n    print(f\"t = {t_stat:.4f}\")\n    # Linear regression\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    coef = model.params['treatment']\n    p = model.pvalues['treatment']\n    print(f\"Regression: coef = {coef:.3f}, p = {p:.4f}\")\n\n\n--- Manual Welch t-test for mrm2 ---\nt = 0.1195\nRegression: coef = 0.014, p = 0.9049\n\n--- Manual Welch t-test for hpa ---\nt = 0.9704\nRegression: coef = 0.637, p = 0.3451\n\n--- Manual Welch t-test for years ---\nt = -1.0909\nRegression: coef = -0.058, p = 0.2700\n\n--- Manual Welch t-test for female ---\nt = -1.7535\nRegression: coef = -0.008, p = 0.0787\n\n--- Manual Welch t-test for couple ---\nt = -0.5823\nRegression: coef = -0.002, p = 0.5594\n\n--- Manual Welch t-test for pwhite ---\nt = -0.5590\nRegression: coef = -0.001, p = 0.5753\n\n--- Manual Welch t-test for pblack ---\nt = 0.0975\nRegression: coef = 0.000, p = 0.9219\n\n--- Manual Welch t-test for ave_hh_sz ---\nt = 0.8234\nRegression: coef = 0.003, p = 0.4098\n\n--- Manual Welch t-test for median_hhincome ---\nt = -0.7433\nRegression: coef = -157.925, p = 0.4583\n\n\n\nimport matplotlib.pyplot as plt\n\n\n\ngroup_totals = df.groupby('treatment')['amount'].sum()\n\ngroup_totals.index = ['Control', 'Treatment']\n\nplt.figure(figsize=(6, 5))\ngroup_totals.plot(kind='bar', color=['skyblue', 'salmon'])\n\nplt.title('Total Donations Raised by Group')\nplt.ylabel('Total Dollars Raised')\nplt.xlabel('Group')\nplt.xticks(rotation=0)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\n\n\n\ncontrol_gave = df[df['treatment'] == 0]['gave'].dropna()\ntreatment_gave = df[df['treatment'] == 1]['gave'].dropna()\n\nt_stat, p_val = ttest_ind(treatment_gave, control_gave, equal_var=False)\nprint(f\"T-test: t = {t_stat:.3f}, p = {p_val:.4f}\")\n\nT-test: t = 3.209, p = 0.0013\n\n\n\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols('gave ~ treatment', data=df).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        17:04:45   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nimport statsmodels.api as sm\n\ndf['intercept'] = 1  # probit model requires intercept manually\nprobit_model = sm.Probit(df['gave'], df[['intercept', 'treatment']]).fit()\nprint(probit_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 23 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        17:04:45   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\nfrom scipy.stats import ttest_ind\n\n# Filter groups\nr1 = df[df['ratio'] == 1]['gave']\nr2 = df[df['ratio2'] == 1]['gave']\nr3 = df[df['ratio3'] == 1]['gave']\n\n# T-tests\nprint(\"T-test: 2:1 vs 1:1\")\nprint(ttest_ind(r2, r1, equal_var=False))\n\nprint(\"\\nT-test: 3:1 vs 1:1\")\nprint(ttest_ind(r3, r1, equal_var=False))\n\nprint(\"\\nT-test: 3:1 vs 2:1\")\nprint(ttest_ind(r3, r2, equal_var=False))\n\nT-test: 2:1 vs 1:1\nTtestResult(statistic=0.965048975142932, pvalue=0.33453078237183076, df=22225.07770983836)\n\nT-test: 3:1 vs 1:1\nTtestResult(statistic=1.0150174470156275, pvalue=0.31010856527625774, df=22215.0529778684)\n\nT-test: 3:1 vs 2:1\nTtestResult(statistic=0.05011581369764474, pvalue=0.9600305476940865, df=22260.84918918778)\n\n\n\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols('gave ~ ratio1 + ratio2 + ratio3', data=df).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0118\nTime:                        17:04:45   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\ncoef_r2 = model.params.get('ratio2', 0)\ncoef_r3 = model.params.get('ratio3', 0)\nprint(f\"\\nDifference 2:1 vs 1:1: {coef_r2:.4f}\")\nprint(f\"Difference 3:1 vs 1:1: {coef_r3:.4f}\")\nprint(f\"Difference 3:1 vs 2:1: {coef_r3 - coef_r2:.4f}\")\n\n\nDifference 2:1 vs 1:1: 0.0048\nDifference 3:1 vs 1:1: 0.0049\nDifference 3:1 vs 2:1: 0.0001\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n\n\ncontrol_amt = df[df['treatment'] == 0]['amount']\ntreatment_amt = df[df['treatment'] == 1]['amount']\nt_stat, p_val = ttest_ind(treatment_amt, control_amt, equal_var=False)\nprint(f\"T-test (all data): t = {t_stat:.3f}, p = {p_val:.4f}\")\n\n# Bivariate regression\nmodel = smf.ols('amount ~ treatment', data=df).fit()\nprint(model.summary())\n\nT-test (all data): t = 1.918, p = 0.0551\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        17:04:45   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\ndf_donors = df[df['amount'] &gt; 0]\n\nc_amt = df_donors[df_donors['treatment'] == 0]['amount']\nt_amt = df_donors[df_donors['treatment'] == 1]['amount']\nt_stat, p_val = ttest_ind(t_amt, c_amt, equal_var=False)\nprint(f\"T-test (donors only): t = {t_stat:.3f}, p = {p_val:.4f}\")\n\nmodel_donors = smf.ols('amount ~ treatment', data=df_donors).fit()\nprint(model_donors.summary())\n\nT-test (donors only): t = -0.585, p = 0.5590\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.561\nTime:                        17:04:45   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nimport matplotlib.pyplot as plt\n\n\ncontrol = df_donors[df_donors['treatment'] == 0]['amount']\ntreatment = df_donors[df_donors['treatment'] == 1]['amount']\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\naxes[0].hist(control, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(control.mean(), color='red', linestyle='dashed', label=f'Mean: ${control.mean():.2f}')\naxes[0].set_title('Control Group - Donation Amounts')\naxes[0].set_xlabel('Donation Amount ($)')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\naxes[1].hist(treatment, bins=30, color='salmon', edgecolor='black')\naxes[1].axvline(treatment.mean(), color='red', linestyle='dashed', label=f'Mean: ${treatment.mean():.2f}')\naxes[1].set_title('Treatment Group - Donation Amounts')\naxes[1].set_xlabel('Donation Amount ($)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nn = 10000\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control\n\ncontrol_draws = np.random.binomial(n=1, p=p_control, size=n)\ntreatment_draws = np.random.binomial(n=1, p=p_treatment, size=n)\n\ndifferences = treatment_draws - control_draws\n\ncumulative_avg_diff = np.cumsum(differences) / np.arange(1, n + 1)\n\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg_diff, label='Cumulative Avg. Difference')\nplt.axhline(true_diff, color='red', linestyle='dashed', label='True Difference = 0.004')\nplt.title(\"Law of Large Numbers: Cumulative Avg. Difference in Donation Rates\")\nplt.xlabel(\"Number of Simulated Pairs\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis simulation shows how the average difference in donation rates between treatment and control groups stabilizes as the sample size increases. Although early values fluctuate due to randomness, the cumulative average converges to the true difference (0.004) as more data is added. This demonstrates the Law of Large Numbers: with enough observations, sample averages reliably approach their expected values.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\ncontrol_p = 0.018\ntreat_p = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\n# Set up plots\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_simulations):\n        control = np.random.binomial(1, control_p, n)\n        treat = np.random.binomial(1, treat_p, n)\n        diffs.append(np.mean(treat) - np.mean(control))\n    \n    # Plot histogram\n    axes[i].hist(diffs, bins=30, color='skyblue', edgecolor='black')\n    axes[i].axvline(0, color='red', linestyle='dashed', label='Zero')\n    axes[i].set_title(f'Sample Size = {n}')\n    axes[i].set_xlabel('Avg. Difference in Means')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs sample size increases, the distribution of average differences becomes more concentrated and symmetric, illustrating the Central Limit Theorem. With small samples (e.g., 50), the spread is wide and variable. But by n = 1000, the distribution is nearly normal and centered around the true mean difference. Importantly, zero is in the tail, not the center, suggesting that the effect of the treatment is consistently positive across simulations."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Estimation of Simple Poisson Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nYour Name\n\n\nJun 10, 2025\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding K-Based Machine Learning: A Study of K-Means Clustering and K-Nearest Neighbors Classification\n\n\n\n\n\n\nScotland Muir\n\n\nJun 10, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nScotland Muir\n\n\nJun 10, 2025\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nScotland\n\n\nMay 28, 2025\n\n\n\n\n\n\nNo matching items"
  }
]