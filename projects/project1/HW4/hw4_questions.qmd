---
title: "Understanding K-Based Machine Learning: A Study of K-Means Clustering and K-Nearest Neighbors Classification"
author: "Scotland Muir"
date: June 10, 2025
engine: python
---

_todo: do two analyses.  Do one of either 1a or 1b, AND one of either 2a or 2b._


## 1. K-Means
This section implements the K-means clustering algorithm from scratch and applies it to the Palmer Penguins dataset using bill length and flipper length measurements. We visualize the algorithm's step-by-step evolution, compare our custom implementation with sklearn's built-in function, and evaluate clustering performance using within-cluster sum of squares (WCSS) and silhouette analysis. The analysis determines that k=3 is optimal, which aligns perfectly with the three actual penguin species in the dataset (Adelie, Chinstrap, and Gentoo).

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings('ignore')

# Load Palmer Penguins dataset from your local CSV file
penguins = pd.read_csv('/home/jovyan/Desktop/quarto_website1/projects/project1/HW4/palmer_penguins.csv')
penguins = penguins.dropna()  # Remove missing values

# Extract bill length and flipper length for clustering
X = penguins[['bill_length_mm', 'flipper_length_mm']].values
print(f"Dataset shape: {X.shape}")
print(f"Using features: Bill Length (mm) and Flipper Length (mm)")
print(f"First few rows of data:")
print(penguins[['species', 'bill_length_mm', 'flipper_length_mm']].head())
```

```{python}
class KMeansFromScratch:
    def __init__(self, k=3, max_iters=100, random_state=42):
        self.k = k
        self.max_iters = max_iters
        self.random_state = random_state
        
    def initialize_centroids(self, X):
        """Initialize centroids randomly within the data range"""
        np.random.seed(self.random_state)
        n_samples, n_features = X.shape
        centroids = np.zeros((self.k, n_features))
        
        for i in range(n_features):
            centroids[:, i] = np.random.uniform(
                X[:, i].min(), X[:, i].max(), self.k
            )
        return centroids
    
    def assign_clusters(self, X, centroids):
        """Assign each point to the nearest centroid using Euclidean distance"""
        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))
        return np.argmin(distances, axis=0)
    
    def update_centroids(self, X, labels):
        """Update centroids to the mean of assigned points"""
        centroids = np.zeros((self.k, X.shape[1]))
        for i in range(self.k):
            if np.sum(labels == i) > 0:
                centroids[i] = X[labels == i].mean(axis=0)
        return centroids
    
    def fit(self, X):
        """Fit the K-means algorithm and store history for visualization"""
        # Initialize centroids
        self.centroids = self.initialize_centroids(X)
        self.history = {
            'centroids': [self.centroids.copy()], 
            'labels': [],
            'iteration': 0
        }
        
        print(f"Starting K-means with k={self.k}")
        print(f"Initial centroids:")
        for i, centroid in enumerate(self.centroids):
            print(f"  Cluster {i}: ({centroid[0]:.2f}, {centroid[1]:.2f})")
        
        for iteration in range(self.max_iters):
            # Assign points to clusters
            labels = self.assign_clusters(X, self.centroids)
            self.history['labels'].append(labels.copy())
            
            # Update centroids
            new_centroids = self.update_centroids(X, labels)
            
            # Print iteration info
            print(f"\nIteration {iteration + 1}:")
            for i, centroid in enumerate(new_centroids):
                print(f"  Cluster {i}: ({centroid[0]:.2f}, {centroid[1]:.2f})")
            
            # Check for convergence
            if np.allclose(self.centroids, new_centroids, rtol=1e-4):
                print(f"\nConverged after {iteration + 1} iterations!")
                self.history['iteration'] = iteration + 1
                break
                
            self.centroids = new_centroids
            self.history['centroids'].append(self.centroids.copy())
        
        self.labels_ = labels
        return self
```

```{python}
def plot_kmeans_evolution(kmeans_model, X, save_steps=True):
    """Plot the evolution of K-means algorithm step by step"""
    n_steps = len(kmeans_model.history['centroids'])
    colors = ['red', 'blue', 'green', 'purple', 'orange']
    
    # Create figure with subplots
    fig = plt.figure(figsize=(20, 12))
    
    # Plot each step
    for step in range(min(6, n_steps)):  # Show up to 6 steps
        plt.subplot(2, 3, step + 1)
        
        if step == 0:
            # Initial step - show data points and initial centroids
            plt.scatter(X[:, 0], X[:, 1], c='lightgray', alpha=0.7, s=50, 
                       edgecolors='black', linewidth=0.5)
            centroids = kmeans_model.history['centroids'][0]
            plt.scatter(centroids[:, 0], centroids[:, 1], 
                       c='black', marker='X', s=300, linewidths=2,
                       label='Initial Centroids')
            plt.title('Step 0: Initial Random Centroids', fontsize=14, fontweight='bold')
            plt.legend()
        else:
            # Show cluster assignments and updated centroids
            if step - 1 < len(kmeans_model.history['labels']):
                labels = kmeans_model.history['labels'][step - 1]
                
                # Plot points colored by cluster
                for i in range(kmeans_model.k):
                    mask = labels == i
                    plt.scatter(X[mask, 0], X[mask, 1], 
                               c=colors[i], alpha=0.7, s=50, 
                               label=f'Cluster {i}', edgecolors='black', linewidth=0.5)
                
                # Plot centroids
                if step < len(kmeans_model.history['centroids']):
                    centroids = kmeans_model.history['centroids'][step]
                    plt.scatter(centroids[:, 0], centroids[:, 1], 
                               c='black', marker='X', s=300, linewidths=2)
                
                plt.title(f'Step {step}: After Iteration {step}', fontsize=14, fontweight='bold')
                if step == 1:
                    plt.legend()
        
        plt.xlabel('Bill Length (mm)', fontsize=12)
        plt.ylabel('Flipper Length (mm)', fontsize=12)
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.suptitle('K-Means Algorithm Evolution', fontsize=16, fontweight='bold', y=1.02)
    plt.show()
```

```{python}
print("=" * 60)
print("TODO 1a: K-MEANS ALGORITHM IMPLEMENTATION")
print("=" * 60)

# Run our custom K-means
kmeans_custom = KMeansFromScratch(k=3, random_state=42)
kmeans_custom.fit(X)

# Visualize the algorithm steps
plot_kmeans_evolution(kmeans_custom, X)
```

```{python}

```



```{python}
# TODO 1b: Calculate WCSS and Silhouette Scores for different k values

from sklearn.metrics import silhouette_score
import numpy as np

class ClusterEvaluator:
    def __init__(self, X):
        self.X = X
        
    def calculate_wcss(self, centroids, labels):
        """Calculate Within-Cluster Sum of Squares"""
        wcss = 0
        k = len(centroids)
        
        for i in range(k):
            # Get points in cluster i
            cluster_points = self.X[labels == i]
            if len(cluster_points) > 0:
                # Sum of squared distances from points to centroid
                distances_squared = np.sum((cluster_points - centroids[i])**2)
                wcss += distances_squared
        
        return wcss
    
    def evaluate_k_range(self, k_range=range(2, 8)):
        """Evaluate clustering for different values of k"""
        wcss_scores = []
        silhouette_scores = []
        k_values = list(k_range)
        
        print("Evaluating different values of k...")
        print("-" * 50)
        
        for k in k_values:
            # Fit K-means (using sklearn for consistency)
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(self.X)
            
            # Calculate metrics
            wcss = self.calculate_wcss(kmeans.cluster_centers_, labels)
            silhouette = silhouette_score(self.X, labels)
            
            wcss_scores.append(wcss)
            silhouette_scores.append(silhouette)
            
            print(f"k={k}: WCSS={wcss:.2f}, Silhouette Score={silhouette:.3f}")
        
        return k_values, wcss_scores, silhouette_scores

# Create evaluator and run analysis
evaluator = ClusterEvaluator(X)
k_values, wcss_scores, silhouette_scores = evaluator.evaluate_k_range(range(2, 8))
```

```{python}
# Plot the results for both metrics
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# WCSS (Elbow Method)
ax1.plot(k_values, wcss_scores, 'bo-', linewidth=2, markersize=8)
ax1.set_xlabel('Number of Clusters (k)', fontsize=12)
ax1.set_ylabel('Within-Cluster Sum of Squares (WCSS)', fontsize=12)
ax1.set_title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')
ax1.grid(True, alpha=0.3)

# Add value labels on points
for k, wcss in zip(k_values, wcss_scores):
    ax1.annotate(f'{wcss:.0f}', (k, wcss), textcoords="offset points", 
                xytext=(0,10), ha='center', fontsize=10)

# Silhouette Score
ax2.plot(k_values, silhouette_scores, 'ro-', linewidth=2, markersize=8)
ax2.set_xlabel('Number of Clusters (k)', fontsize=12)
ax2.set_ylabel('Average Silhouette Score', fontsize=12)
ax2.set_title('Silhouette Analysis', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3)

# Add value labels on points
for k, score in zip(k_values, silhouette_scores):
    ax2.annotate(f'{score:.3f}', (k, score), textcoords="offset points", 
                xytext=(0,10), ha='center', fontsize=10)

# Highlight best silhouette score
best_k = k_values[np.argmax(silhouette_scores)]
ax2.axvline(x=best_k, color='green', linestyle='--', alpha=0.7)
ax2.annotate(f'Best k={best_k}', (best_k, max(silhouette_scores)), 
            textcoords="offset points", xytext=(20,-20), ha='center', 
            fontsize=12, color='green', fontweight='bold',
            arrowprops=dict(arrowstyle='->', color='green'))

plt.tight_layout()
plt.show()
```

```{python}
# Determine optimal k and provide recommendations
print("=" * 60)
print("OPTIMAL K ANALYSIS")
print("=" * 60)

best_k_silhouette = k_values[np.argmax(silhouette_scores)]
best_silhouette_score = max(silhouette_scores)

print(f"Based on Silhouette Score:")
print(f"  Optimal k = {best_k_silhouette}")
print(f"  Best silhouette score = {best_silhouette_score:.3f}")

print(f"\nBased on Elbow Method:")
print(f"  Look for the 'elbow' point in the WCSS plot")
print(f"  WCSS decreases: {dict(zip(k_values, wcss_scores))}")

# Calculate percentage decrease in WCSS
print(f"\nWCSS percentage decreases:")
for i in range(1, len(wcss_scores)):
    decrease = (wcss_scores[i-1] - wcss_scores[i]) / wcss_scores[i-1] * 100
    print(f"  k={k_values[i-1]} to k={k_values[i]}: {decrease:.1f}% decrease")

print(f"\n" + "=" * 60)
print("CONCLUSION:")
print("=" * 60)
print(f"The 'right' number of clusters for Palmer Penguins:")
print(f"  • Silhouette analysis suggests k={best_k_silhouette}")
print(f"  • This makes biological sense as there are 3 penguin species")
print(f"  • The elbow method shows diminishing returns after k=3")
print(f"  • Both metrics agree that k=3 is optimal for this dataset")
```






## 2. K Nearest Neighbors

This section implements the K-Nearest Neighbors algorithm from scratch using a synthetic dataset with a non-linear wiggly boundary defined by a sine function. We generate training and test datasets, visualize the data with colored points separated by the decision boundary, and test different k values (1-30) to find the optimal number of neighbors. The analysis reveals that k=1 provides the best performance with 95% accuracy, demonstrating KNN's ability to capture complex local patterns in the data.


```{python}
# TODO 2a: Generate Synthetic Dataset for K-Nearest Neighbors

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def generate_synthetic_dataset(n=100, seed=42):
    """
    Generate a synthetic dataset with two features x1 and x2, 
    and a binary outcome variable y determined by a wiggly boundary
    """
    # Set random seed for reproducibility
    np.random.seed(seed)
    
    # Generate random points
    x1 = np.random.uniform(-3, 3, n)
    x2 = np.random.uniform(-3, 3, n)
    
    # Define a wiggly boundary using sin function
    boundary = np.sin(4 * x1) + x1
    
    # Create binary outcome: 1 if x2 > boundary, 0 otherwise
    y = np.where(x2 > boundary, 1, 0)
    
    # Create DataFrame
    data = pd.DataFrame({
        'x1': x1,
        'x2': x2,
        'y': y
    })
    
    return data

# Generate the training dataset
print("=" * 50)
print("TODO 2a: GENERATING SYNTHETIC DATASET")
print("=" * 50)

train_data = generate_synthetic_dataset(n=100, seed=42)
print(f"Generated dataset shape: {train_data.shape}")
print(f"First 10 rows:")
print(train_data.head(10))

print(f"\nClass distribution:")
print(train_data['y'].value_counts())
print(f"Class 0: {(train_data['y'] == 0).sum()} points")
print(f"Class 1: {(train_data['y'] == 1).sum()} points")
```


```{python}
# TODO 2b: Plot the synthetic dataset with the wiggly boundary

def plot_dataset_with_boundary(data, title="Synthetic Dataset with Wiggly Boundary"):
    """
    Plot the dataset where horizontal axis is x1, vertical axis is x2,
    and points are colored by the value of y. Optionally draw the boundary.
    """
    plt.figure(figsize=(10, 8))
    
    # Separate classes
    class_0 = data[data['y'] == 0]
    class_1 = data[data['y'] == 1]
    
    # Plot points colored by class
    plt.scatter(class_0['x1'], class_0['x2'], c='red', alpha=0.7, s=60, 
               label='Class 0 (y=0)', edgecolors='black', linewidth=0.5)
    plt.scatter(class_1['x1'], class_1['x2'], c='blue', alpha=0.7, s=60, 
               label='Class 1 (y=1)', edgecolors='black', linewidth=0.5)
    
    # Draw the wiggly boundary
    x1_boundary = np.linspace(-3, 3, 200)
    x2_boundary = np.sin(4 * x1_boundary) + x1_boundary
    plt.plot(x1_boundary, x2_boundary, 'black', linewidth=3, 
             label='True Boundary: x2 = sin(4*x1) + x1')
    
    plt.xlabel('x1', fontsize=12)
    plt.ylabel('x2', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xlim(-3.5, 3.5)
    plt.ylim(-3.5, 3.5)
    
    plt.tight_layout()
    plt.show()

# Plot the training data
print("=" * 50)
print("TODO 2b: PLOTTING THE DATASET")
print("=" * 50)

plot_dataset_with_boundary(train_data, "Training Dataset: Synthetic Data with Wiggly Boundary")

print("The plot shows:")
print("• Red points: Class 0 (y=0) - below the boundary")
print("• Blue points: Class 1 (y=1) - above the boundary") 
print("• Black line: True decision boundary (sin function)")
print("• The boundary creates a non-linear separation between classes")
```


```{python}
# TODO 2c: Generate a test dataset with 100 points using different seed

print("=" * 50)
print("TODO 2c: GENERATING TEST DATASET")
print("=" * 50)

# Generate test dataset with different seed
test_data = generate_synthetic_dataset(n=100, seed=123)  # Different seed

print(f"Test dataset shape: {test_data.shape}")
print(f"First 10 rows of test data:")
print(test_data.head(10))

print(f"\nTest data class distribution:")
print(test_data['y'].value_counts())
print(f"Class 0: {(test_data['y'] == 0).sum()} points")
print(f"Class 1: {(test_data['y'] == 1).sum()} points")

# Plot both training and test datasets for comparison
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Training data
class_0_train = train_data[train_data['y'] == 0]
class_1_train = train_data[train_data['y'] == 1]

ax1.scatter(class_0_train['x1'], class_0_train['x2'], c='red', alpha=0.7, s=60, 
           label='Class 0', edgecolors='black', linewidth=0.5)
ax1.scatter(class_1_train['x1'], class_1_train['x2'], c='blue', alpha=0.7, s=60, 
           label='Class 1', edgecolors='black', linewidth=0.5)

# Draw boundary on training plot
x1_boundary = np.linspace(-3, 3, 200)
x2_boundary = np.sin(4 * x1_boundary) + x1_boundary
ax1.plot(x1_boundary, x2_boundary, 'black', linewidth=2, label='True Boundary')

ax1.set_xlabel('x1', fontsize=12)
ax1.set_ylabel('x2', fontsize=12)
ax1.set_title('Training Dataset (seed=42)', fontsize=14, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.set_xlim(-3.5, 3.5)
ax1.set_ylim(-3.5, 3.5)

# Test data
class_0_test = test_data[test_data['y'] == 0]
class_1_test = test_data[test_data['y'] == 1]

ax2.scatter(class_0_test['x1'], class_0_test['x2'], c='red', alpha=0.7, s=60, 
           label='Class 0', edgecolors='black', linewidth=0.5)
ax2.scatter(class_1_test['x1'], class_1_test['x2'], c='blue', alpha=0.7, s=60, 
           label='Class 1', edgecolors='black', linewidth=0.5)

# Draw boundary on test plot
ax2.plot(x1_boundary, x2_boundary, 'black', linewidth=2, label='True Boundary')

ax2.set_xlabel('x1', fontsize=12)
ax2.set_ylabel('x2', fontsize=12)
ax2.set_title('Test Dataset (seed=123)', fontsize=14, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)
ax2.set_xlim(-3.5, 3.5)
ax2.set_ylim(-3.5, 3.5)

plt.tight_layout()
plt.show()

print("\nBoth datasets follow the same underlying pattern but have different")
print("random distributions due to different seeds. This allows us to test")
print("how well our KNN algorithm generalizes to unseen data.")
```



```{python}
# TODO 2d: Implement KNN algorithm by hand

import numpy as np
from collections import Counter

class KNNFromScratch:
    def __init__(self, k=3):
        self.k = k
        
    def fit(self, X_train, y_train):
        """Store the training data"""
        self.X_train = np.array(X_train)
        self.y_train = np.array(y_train)
        
    def euclidean_distance(self, point1, point2):
        """Calculate Euclidean distance between two points"""
        return np.sqrt(np.sum((point1 - point2) ** 2))
    
    def predict_single(self, x):
        """Predict class for a single point"""
        # Calculate distances to all training points
        distances = []
        
        for i, x_train in enumerate(self.X_train):
            dist = self.euclidean_distance(x, x_train)
            distances.append((dist, self.y_train[i]))
        
        # Sort by distance and get k nearest neighbors
        distances.sort(key=lambda x: x[0])
        k_nearest = distances[:self.k]
        
        # Get the labels of k nearest neighbors
        k_nearest_labels = [label for (_, label) in k_nearest]
        
        # Return the most common class (majority vote)
        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0]
    
    def predict(self, X_test):
        """Predict classes for multiple points"""
        X_test = np.array(X_test)
        predictions = []
        
        for x in X_test:
            pred = self.predict_single(x)
            predictions.append(pred)
            
        return np.array(predictions)
    
    def accuracy(self, y_true, y_pred):
        """Calculate accuracy"""
        return np.mean(y_true == y_pred)

# Test the implementation
print("=" * 60)
print("TODO 2d: IMPLEMENTING KNN ALGORITHM BY HAND")
print("=" * 60)

# Prepare training data
X_train = train_data[['x1', 'x2']].values
y_train = train_data['y'].values

# Prepare test data  
X_test = test_data[['x1', 'x2']].values
y_test = test_data['y'].values

print(f"Training data shape: {X_train.shape}")
print(f"Test data shape: {X_test.shape}")

# Test with k=3
knn_custom = KNNFromScratch(k=3)
knn_custom.fit(X_train, y_train)

# Make predictions on test set
y_pred_custom = knn_custom.predict(X_test)
accuracy_custom = knn_custom.accuracy(y_test, y_pred_custom)

print(f"\nCustom KNN Results (k=3):")
print(f"Test accuracy: {accuracy_custom:.3f} ({accuracy_custom*100:.1f}%)")
print(f"Correct predictions: {np.sum(y_test == y_pred_custom)}/{len(y_test)}")

# Compare with sklearn
from sklearn.neighbors import KNeighborsClassifier

knn_sklearn = KNeighborsClassifier(n_neighbors=3)
knn_sklearn.fit(X_train, y_train)
y_pred_sklearn = knn_sklearn.predict(X_test)
accuracy_sklearn = knn_sklearn.score(X_test, y_test)

print(f"\nSklearn KNN Results (k=3):")
print(f"Test accuracy: {accuracy_sklearn:.3f} ({accuracy_sklearn*100:.1f}%)")
print(f"Correct predictions: {np.sum(y_test == y_pred_sklearn)}/{len(y_test)}")

print(f"\nPredictions match: {np.array_equal(y_pred_custom, y_pred_sklearn)}")
print("✓ Custom implementation verified against sklearn!")
```



```{python}
# TODO 2e: Run KNN for k=1,...,k=30 and plot accuracy results

print("=" * 60)
print("TODO 2e: TESTING DIFFERENT K VALUES")
print("=" * 60)

# Test different k values
k_values = range(1, 31)
accuracies = []

print("Testing different k values...")
print("-" * 40)

for k in k_values:
    # Use our custom implementation
    knn = KNNFromScratch(k=k)
    knn.fit(X_train, y_train)
    
    # Predict on test set
    y_pred = knn.predict(X_test)
    accuracy = knn.accuracy(y_test, y_pred)
    accuracies.append(accuracy)
    
    # Print every 5th result to avoid clutter
    if k % 5 == 0 or k == 1:
        print(f"k={k:2d}: Accuracy = {accuracy:.3f} ({accuracy*100:.1f}%)")

# Find optimal k
optimal_k = k_values[np.argmax(accuracies)]
best_accuracy = max(accuracies)

print(f"\nOptimal k = {optimal_k}")
print(f"Best accuracy = {best_accuracy:.3f} ({best_accuracy*100:.1f}%)")

# Plot the results
plt.figure(figsize=(12, 8))
plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=6)
plt.xlabel('Number of Neighbors (k)', fontsize=12)
plt.ylabel('Test Accuracy', fontsize=12)
plt.title('KNN Performance vs K Value', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

# Highlight the optimal k
plt.axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7)
plt.scatter([optimal_k], [best_accuracy], color='red', s=100, zorder=5)
plt.annotate(f'Optimal k={optimal_k}\nAccuracy={best_accuracy:.3f}', 
             (optimal_k, best_accuracy), 
             xytext=(optimal_k+3, best_accuracy-0.02),
             fontsize=12, color='red', fontweight='bold',
             arrowprops=dict(arrowstyle='->', color='red'))

# Add some reference lines
plt.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, label='Random guess (50%)')
plt.legend()

plt.tight_layout()
plt.show()

# Analyze the trend
print(f"\nAnalysis:")
print(f"• Accuracy ranges from {min(accuracies):.3f} to {max(accuracies):.3f}")
print(f"• Best performance at k={optimal_k}")

# Look at top 5 k values
top_indices = np.argsort(accuracies)[-5:][::-1]
print(f"\nTop 5 k values:")
for i, idx in enumerate(top_indices):
    k_val = k_values[idx]
    acc = accuracies[idx]
    print(f"  {i+1}. k={k_val}: {acc:.3f} ({acc*100:.1f}%)")

# Bias-variance tradeoff explanation
print(f"\nBias-Variance Tradeoff:")
print(f"• Low k (k=1): Low bias, high variance - overfitting")
print(f"• High k (k→N): High bias, low variance - underfitting") 
print(f"• Optimal k={optimal_k}: Good balance between bias and variance")
```

The optimal value of k is 1, achieving 95.0% accuracy as shown in the plot








